{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78103890",
   "metadata": {},
   "source": [
    "# Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e942f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "761b19dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = '/home/gonzalez/Desktop/age_gender/age_gender.csv' # old data\n",
    "# url = 'https://www.kaggle.com/code/shahraizanwar/age-gender-ethnicity-prediction/data?select=age_gender.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403e8667",
   "metadata": {},
   "source": [
    "Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccee75e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/gonzalez/code/jagonzalezj/age_gender/raw_data/new_data.csv' \n",
    "data = pd.read_csv(data_path, encoding='utf-8')\n",
    "\n",
    "#data['test_age']=0==data['age']\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42809f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.test_age.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af0c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def compute_gender_baseline(df):\n",
    "    df['test_gender']=(data['gender']==0)\n",
    "    return df['test_gender'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d735761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute_gender_baseline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9fd05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/gonzalez/Desktop/age_gender/age_gender.csv' # old data\n",
    "data = pd.read_csv(data_path)\n",
    "data['pixels']=data['pixels'].apply(lambda x:  np.array(x.split(), dtype=\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22378fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4946/3047410609.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['pixels'][j]=data['pixels'][j]/255\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(data)):\n",
    "    data['pixels'][j]=data['pixels'][j]/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d08f5038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>gender</th>\n",
       "      <th>img_name</th>\n",
       "      <th>pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20161219203650636.jpg.chip.jpg</td>\n",
       "      <td>[0.5058824, 0.5019608, 0.5019608, 0.49411765, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20161219222752047.jpg.chip.jpg</td>\n",
       "      <td>[0.6431373, 0.2901961, 0.43529412, 0.65882355,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20161219222832191.jpg.chip.jpg</td>\n",
       "      <td>[0.2627451, 0.27450982, 0.2784314, 0.27450982,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20161220144911423.jpg.chip.jpg</td>\n",
       "      <td>[0.75686276, 0.77254903, 0.7764706, 0.78431374...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20161220144914327.jpg.chip.jpg</td>\n",
       "      <td>[0.7921569, 0.8039216, 0.81960785, 0.8235294, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23700</th>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20170120221920654.jpg.chip.jpg</td>\n",
       "      <td>[0.49803922, 0.39215687, 0.36862746, 0.3176470...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23701</th>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20170120134639935.jpg.chip.jpg</td>\n",
       "      <td>[0.09019608, 0.10980392, 0.1254902, 0.13725491...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23702</th>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20170110182418864.jpg.chip.jpg</td>\n",
       "      <td>[0.23137255, 0.19607843, 0.14509805, 0.1568627...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23703</th>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20170117195405372.jpg.chip.jpg</td>\n",
       "      <td>[0.1764706, 0.42352942, 0.47058824, 0.6117647,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23704</th>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20170110182052119.jpg.chip.jpg</td>\n",
       "      <td>[0.6117647, 0.6313726, 0.627451, 0.64705884, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23705 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  ethnicity  gender                        img_name  \\\n",
       "0        1          2       0  20161219203650636.jpg.chip.jpg   \n",
       "1        1          2       0  20161219222752047.jpg.chip.jpg   \n",
       "2        1          2       0  20161219222832191.jpg.chip.jpg   \n",
       "3        1          2       0  20161220144911423.jpg.chip.jpg   \n",
       "4        1          2       0  20161220144914327.jpg.chip.jpg   \n",
       "...    ...        ...     ...                             ...   \n",
       "23700   99          0       1  20170120221920654.jpg.chip.jpg   \n",
       "23701   99          1       1  20170120134639935.jpg.chip.jpg   \n",
       "23702   99          2       1  20170110182418864.jpg.chip.jpg   \n",
       "23703   99          2       1  20170117195405372.jpg.chip.jpg   \n",
       "23704   99          0       1  20170110182052119.jpg.chip.jpg   \n",
       "\n",
       "                                                  pixels  \n",
       "0      [0.5058824, 0.5019608, 0.5019608, 0.49411765, ...  \n",
       "1      [0.6431373, 0.2901961, 0.43529412, 0.65882355,...  \n",
       "2      [0.2627451, 0.27450982, 0.2784314, 0.27450982,...  \n",
       "3      [0.75686276, 0.77254903, 0.7764706, 0.78431374...  \n",
       "4      [0.7921569, 0.8039216, 0.81960785, 0.8235294, ...  \n",
       "...                                                  ...  \n",
       "23700  [0.49803922, 0.39215687, 0.36862746, 0.3176470...  \n",
       "23701  [0.09019608, 0.10980392, 0.1254902, 0.13725491...  \n",
       "23702  [0.23137255, 0.19607843, 0.14509805, 0.1568627...  \n",
       "23703  [0.1764706, 0.42352942, 0.47058824, 0.6117647,...  \n",
       "23704  [0.6117647, 0.6313726, 0.627451, 0.64705884, 0...  \n",
       "\n",
       "[23705 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c219bbff",
   "metadata": {},
   "source": [
    "# processing color data from Pierre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61de7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    " # new data from Pierre\n",
    "data_path = '/home/gonzalez/code/jagonzalezj/age_gender/raw_data/new_data.csv' \n",
    "data = pd.read_csv(data_path, encoding='utf-8')\n",
    "\n",
    "lola =[]   \n",
    "for i in range(len(data['image'])):\n",
    "    a = data['image'][i].replace('[',',').replace(']',',').replace(',','').split()\n",
    "    lola.append([int(j) for j in a])\n",
    "    \n",
    "data['image']=lola\n",
    "data.columns=['age', 'gender', 'ethnicity', 'pixels']\n",
    "\n",
    "data['ethnicity'].unique()[5:]\n",
    "for items in data['ethnicity'].unique()[5:]:\n",
    "    data = data.drop(data[data.ethnicity==items].index).copy()\n",
    "\n",
    "data = data.dropna()\n",
    "data.reindex()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec1061",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Loading data from google cloud to google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612cad54",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive')\n",
    "df=pd.read_csv('gdrive/My Drive/age_gender.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de094b9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Transforming the pixels data type into a list of float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67406a11",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# images =[]\n",
    "# for fotos in range(len(data['pixels'])):\n",
    "#     X = data['pixels'][fotos].split(\" \")\n",
    "#     X = list(map(int, X))\n",
    "#     images.append(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c382e670",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# x = np.reshape(images[5000], (48,48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2034415d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data['pixels']=data['pixels'].apply(lambda x:  np.array(x.split(), dtype=\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1367365",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "blob = data['pixels'][0].reshape(48,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c74751",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(blob, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a953707",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#sns.displot(data['ethnicity']),\n",
    "#sns.displot(data['gender']), \n",
    "#sns.displot(data['age']);\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 5))\n",
    "sns.histplot(ax=axes[0], x=data['age']);\n",
    "sns.histplot(ax=axes[1], x=data['ethnicity']);\n",
    "sns.histplot(ax=axes[2], x=data['gender']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3251d861",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Working with the age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2f14c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# list the number of counts per age\n",
    "ages = data['age'].unique()\n",
    "counts = []\n",
    "for age in ages:\n",
    "    counts.append(np.count_nonzero(data['age']==age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f55ada",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# table with the first 15 most dense samples regarding age\n",
    "type(ages), type(counts)\n",
    "s =pd.DataFrame([ages.T, np.array(counts).T],['ages', 'counts'])\n",
    "s=s.transpose()\n",
    "more_dense = s.sort_values(by=['counts'], ascending=False)\n",
    "more_dense.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a8a04",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = data.drop(data[data.age==29].index).copy()\n",
    "data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d30e9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.histplot(data.age);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a875dbe4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The filter of Pierre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce2309",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_clean = data.copy()\n",
    "data['points_bin'] = pd.qcut(data_clean['age'], q=10)\n",
    "\n",
    "#view updated DataFrame\n",
    "print(data)\n",
    "data['points_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039097f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2925d612",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# External image manipulation funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b63a3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde420cb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get image\n",
    "#locattion = \"/home/gonzalez/foto.jpg\" # javier\n",
    "#locattion = \"/home/gonzalez/Paul.jpeg\" # Paul\n",
    "#locattion = \"/home/gonzalez/Konstantine.jpeg\"\n",
    "locattion = \"/home/gonzalez/ping.jpg\" # Paul\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dd0703",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#base_dir = os.path.dirname(locattion)\n",
    "#image = cv2.imread(\"/home/gonzalez/foto.jpg\")\n",
    "#plt.imshow(image, cmap='gray');\n",
    "#print(f'==> image resolution {image.shape}')\n",
    "\n",
    "# from PIL import Image           # this can be used to rotate images\n",
    "# image = Image.open(locattion)\n",
    "\n",
    "# (h, w) = image.shape[:2]\n",
    "# blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "# #blob.shape\n",
    "# blob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3935ab",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "imagePath=locattion\n",
    "image = cv2.imread(imagePath)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "faces = faceCascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor=1.3,\n",
    "    minNeighbors=3,\n",
    "    minSize=(30, 30)\n",
    ")\n",
    "\n",
    "print(\"[INFO] Found {0} Faces.\".format(len(faces)))\n",
    "\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0),1)\n",
    "    roi_color = image[y:y + h, x:x + w]\n",
    "    #print(\"[INFO] Object found. Saving locally.\")\n",
    "    #cv2.imwrite(str(w) + str(h) + '_faces.jpg', roi_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df484e00",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(16,16))\n",
    "\n",
    "ax1.imshow(image) # original image\n",
    "\n",
    "ax2.imshow(roi_color) # recorted original image\n",
    "roi_color.shape\n",
    "\n",
    "img = np.mean(roi_color, axis=2) # black and white image\n",
    "ax3.imshow(img, cmap='gray');\n",
    "\n",
    "img=img[2:,2:]  # remove red line effect\n",
    "\n",
    "res_final = cv2.resize(img, dsize=(48, 48), interpolation=cv2.INTER_LINEAR)\n",
    "ax4.imshow(res_final, cmap='gray');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e2f5f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res_final.shape\n",
    "res_final_ready = np.reshape(res_final, (-1, 48, 48,1))\n",
    "res_final_ready.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c6649",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model.predict(res_final_ready)\n",
    "# index = np.where(model.predict(res_final_ready)==(model.predict(res_final_ready).max()))\n",
    "# index[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48cb79",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#model = models.load_model('Model48_datafiltered/')\n",
    "int(model.predict(res_final_ready)[0][0])\n",
    "\n",
    "#model.predict(res_final_ready)\n",
    "#index = np.where(model.predict(res_final_ready)==(model.predict(res_final_ready).max()))\n",
    "#print(f'slot number {index[1][0]}, correspond to range {index[1][0]*step_size-step_size} to {index[1][0]*step_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7755ee0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# resize image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5413b2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# image conver to black and white\n",
    "#image = cv2.imread(imagePath)\n",
    "#gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475cf968",
   "metadata": {},
   "source": [
    "# Function for transforming data numbers into data range classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a608365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize age per range:\n",
    "def age_categorize(input_list, age_step=10):\n",
    "    '''\n",
    "    Enter the list of age into input_list and the age steps\n",
    "    with : age_step = 5;  age = 4   =>  1-5\n",
    "                          age = 12  =>  10-15                        \n",
    "    '''\n",
    "    \n",
    "    cat_age = []\n",
    "    for age in input_list:\n",
    "        \n",
    "        a = float(age)/float(age_step)\n",
    "        \n",
    "        if a > 1:\n",
    "            entero = int(a)\n",
    "            coma = a-entero\n",
    "            \n",
    "            if coma > 0:\n",
    "                entero = entero+1\n",
    "            \n",
    "            max = entero * age_step\n",
    "            min = max-(age_step-1)     \n",
    "            #cat_age.append(f'{min} to {max}')   # if the output is in the real intervale\n",
    "            cat_age.append(int(max/age_step)-1)  # if the output is in categorical int number\n",
    "        else:\n",
    "            min = 1\n",
    "            max = age_step\n",
    "            #cat_age.append(f'{min} to {max}')    # if the output is in the real intervale   \n",
    "            cat_age.append(int(max/age_step)-1)   # if the output is in categorical int number\n",
    "            \n",
    "    return cat_age\n",
    "\n",
    "\n",
    "def age_categorize_custom(input_list, custom = [0, 15, 35, 60, 150]):\n",
    "    category = list(range(0,len(custom)))\n",
    "    data['age'].unique()\n",
    "    cat_age = []\n",
    "\n",
    "    for edad in data['age']:\n",
    "        category = list(range(0,len(custom)))\n",
    "        for cate in category:\n",
    "            if edad<custom[cate]:\n",
    "                #print(edad, cate, custom[cate-1], custom[cate])\n",
    "                cat_age.append(cate)\n",
    "                break\n",
    "    return cat_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50ff74a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6033ea",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Here we go with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f3f14",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers, models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e6a003",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X = data['pixels'].tolist()\n",
    "# X = np.reshape(X, (-1, 48, 48,1))\n",
    "\n",
    "\n",
    "X = data['pixels'].tolist()\n",
    "X = np.reshape(X, (-1, 48, 48,3))\n",
    "\n",
    "y=data['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43503b3e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5110f4f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# train_datagen=ImageDataGenerator(rescale=1/255)\n",
    "# train_generator_age=train_datagen.flow(\n",
    "#     X_train ,y_train ,batch_size=32)\n",
    "\n",
    "# test_datagen=ImageDataGenerator(rescale=1/255)\n",
    "# test_generator_age=test_datagen.flow(\n",
    "#     X_test ,y_test ,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a9196d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def initialize_model(numb_int, numb_out):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(48,48,numb_int)))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "   \n",
    "    model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))          \n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='relu'))\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463a3e3",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = initialize_model(X.shape[-1], y.shape[-1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a62db1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de183bdf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='mae', patience=6, restore_best_weights=True)\n",
    "\n",
    "# earlystop=EarlyStopping(patience=6)\n",
    "# learning_rate_reduction=ReduceLROnPlateau(\n",
    "#     monitor='val_acc',\n",
    "#     patience= 3,\n",
    "#     verbose=1,\n",
    "# )\n",
    "# callbacks = [earlystop, learning_rate_reduction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216753a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66b1fda",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# history_age = model.fit(\n",
    "#     train_generator_age, \n",
    "#     epochs= 60,\n",
    "#     validation_data= test_generator_age,\n",
    "#     callbacks= callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839ca07",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#history = model.fit(X_train, y_train, epochs=40, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d014940e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X, y, validation_split=0.3, epochs=40, callbacks=[es], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ca500e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#history.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0f22a2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss']);\n",
    "plt.plot(history.history['mae']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d35cd0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "models.save_model(model, 'Model48_linearColor')\n",
    "#model = models.load_model('Model48')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986d211d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b4158",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n=142\n",
    "plt.imshow(X[n] );\n",
    "data.iloc[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c234dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#out= model.predict(X_test)\n",
    "try_inp = np.expand_dims(X[n], axis=0)\n",
    "model.predict(try_inp)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f0f02f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.shape(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01508e8a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# MODEL USING DATA BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbf9a64",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers, models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57b0e15",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "step_size = 5\n",
    "input_list = data['age']\n",
    "cat = age_categorize(input_list, step_size)\n",
    "#pd.DataFrame(cat, data['age'].values).sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ea20a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add categorical age clasification to original dataframe\n",
    "data['class_age']=cat\n",
    "#data[['age','class_age']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4443bf5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.histplot(data['class_age']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df7c87",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### perform one-hot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse = False)\n",
    "ohe.fit(data[['class_age']])\n",
    "class_age_encoded = ohe.transform(data[['class_age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd15671",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9051a97",
   "metadata": {
    "hidden": true
   },
   "source": [
    " Using Pierre distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39f262b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pierre distribution\n",
    "data_clean = data.copy()\n",
    "data['points_bin'] = pd.qcut(data_clean['age'], q=10)\n",
    "\n",
    "#view updated DataFrame\n",
    "print(data)\n",
    "data['points_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e401d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# perform one-hot encoder to the Pierre distribution\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse = False)\n",
    "ohe.fit(data[['points_bin']])\n",
    "class_age_encoded = ohe.transform(data[['points_bin']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418f2d9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for elements in range(class_age_encoded.shape[1]):      # =====> THIS IS NEED WHATHERVER HOT ENCODER USED  <=====\n",
    "    data[str(elements)]=class_age_encoded[:,elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a0cc5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y=data.drop(columns=['age','ethnicity','gender', 'pixels', 'points_bin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add11fe",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finished Pierre encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d472cd50",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#y=data.drop(columns=['age','ethnicity','gender', 'pixels', 'class_age'])\n",
    "#y=data.drop(columns=['age','ethnicity','gender', 'img_name', 'pixels', 'points_bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4a07d1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = data['pixels'].tolist()\n",
    "X = np.reshape(X, (-1, 48, 48,3))\n",
    "\n",
    "y = class_age_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b1c397",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d2766",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def initialize_model_catgorical(numb_int, numb_out):\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32,(3,3), padding='same',activation='relu',input_shape=(48,48,numb_int)))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    model.add(layers.Conv2D(32,(3,3), padding='same',activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    model.add(layers.Conv2D(64,(3,3), padding='same',activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128,activation='relu'))\n",
    "   \n",
    "    #model.add(layers.Flatten())\n",
    "    #model.add(layers.Dense(128,activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(numb_out, activation='softmax'))   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b5318",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#model = initialize_model_catgorical()\n",
    "model = initialize_model_catgorical(X.shape[-1], y.shape[-1])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d274b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam' ,loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d3c0f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='accuracy', patience=6, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda9868",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "history_cat = model.fit(X_train,y_train, validation_split=0.3, epochs=50, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afad490",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#history_cat = model.fit(X, y, validation_split=0.3, epochs=40, callbacks=[es], batch_size=32)  # the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6883912",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "models.save_model(model, 'Model48_categorical_ColorData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4b4d6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history_cat.history['val_accuracy']);\n",
    "plt.plot(history_cat.history['accuracy']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dda7242",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece4e418",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n=244\n",
    "plt.imshow(X[n], cmap='gray');\n",
    "#np.where(y.iloc[n]==1)[0]\n",
    "print(f\"real age is {data.iloc[n]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed54e9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try_inp = np.expand_dims(X[n], axis=0)\n",
    "model.predict(try_inp).max()\n",
    "index = np.where(model.predict(try_inp)==(model.predict(try_inp).max()))\n",
    "index[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ded3c6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.predict(try_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837fcf02",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try_inp = np.expand_dims(X[n], axis=0)\n",
    "model.predict(try_inp).max()\n",
    "index = np.where(model.predict(try_inp)==(model.predict(try_inp).max()))\n",
    "print(f'slot number {index[1][0]}, correspond to range {index[1][0]*step_size-step_size} to {index[1][0]*step_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64598d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# best results using colab with regression on virgen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bddf859",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import csv\n",
    "\n",
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive')\n",
    "data=pd.read_csv('gdrive/My Drive/age_gender.csv')\n",
    "\n",
    "data['pixels']=data['pixels'].apply(lambda x:  np.array(x.split(), dtype=\"float32\"))\n",
    "\n",
    "from tensorflow.keras import Sequential, layers, models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['pixels'].tolist()\n",
    "X = np.reshape(X, (-1, 48, 48,1))\n",
    "\n",
    "y = data['age']\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(48,48,1)))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "   \n",
    "    model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))          \n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='relu'))\n",
    "   \n",
    "    return model\n",
    "\n",
    "model = initialize_model()\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, callbacks=[es])\n",
    "\n",
    "plt.plot(history.history['loss']);\n",
    "plt.plot(history.history['mae']);\n",
    "\n",
    "n=5\n",
    "out = np.reshape(X_test[n], (48, 48))\n",
    "plt.imshow(out, cmap='gray');\n",
    "y_test.iloc[n]\n",
    "\n",
    "try_inp = np.expand_dims(X_test[n], axis=0)\n",
    "model.predict(try_inp)[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4d127",
   "metadata": {},
   "source": [
    "# Combination of categorical plus linear for BW images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b6e942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 16:07:48.565116: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/gonzalez/TOOLS/elmer/install//lib\n",
      "2022-06-05 16:07:48.565138: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import csv\n",
    "from tensorflow.keras import Sequential, layers, models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c3437",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7299bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_regression():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(48,48,1)))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "   \n",
    "    model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))          \n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='relu'))\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd67a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_catgorical(numb_int, numb_out):\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32,(3,3), padding='same',activation='relu',input_shape=(48,48,numb_int)))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    model.add(layers.Conv2D(32,(3,3), padding='same',activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    model.add(layers.Conv2D(64,(3,3), padding='same',activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(64,(3,3), padding='same',activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(layers.Conv2D(32,(3,3), padding='same',activation='relu',input_shape=(48,48,numb_int)))\n",
    "#     model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "#     model.add(layers.Conv2D(32,(3,3), padding='same',activation='relu'))\n",
    "#     model.add(layers.MaxPooling2D(2,2))\n",
    "    \n",
    "\n",
    "    model.add(layers.Conv2D(320,(3,3), padding='same',activation='relu'))\n",
    "    #model.add(layers.MaxPooling2D(2,2))\n",
    "    #model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(layers.Conv2D(320,(3,3), padding='same',activation='relu'))\n",
    "    #model.add(layers.MaxPooling2D(2,2))\n",
    "    #model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(520,(3,3), padding='same',activation='relu'))\n",
    "    #model.add(layers.MaxPooling2D(2,2))\n",
    "    #model.add(layers.Dropout(0.2))\n",
    "\n",
    "#     model.add(layers.Conv2D(160,(3,3), padding='same',activation='relu'))\n",
    "#     model.add(layers.MaxPooling2D(2,2))\n",
    "#     model.add(layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128,activation='relu'))\n",
    "   \n",
    "#     model.add(layers.Flatten())\n",
    "#     model.add(layers.Dense(128,activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(numb_out, activation='softmax'))   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712f0d74",
   "metadata": {},
   "source": [
    "## load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd26d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/gonzalez/Desktop/age_gender/age_gender.csv' # old data\n",
    "data = pd.read_csv(data_path)\n",
    "data['pixels']=data['pixels'].apply(lambda x:  np.array(x.split(), dtype=\"float32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43684fe8",
   "metadata": {},
   "source": [
    "## FILTERING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c8d9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the number of counts per age\n",
    "ages = data['age'].unique()\n",
    "counts = []\n",
    "for age in ages:\n",
    "    counts.append(np.count_nonzero(data['age']==age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ab46acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ages</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>2197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>35</td>\n",
       "      <td>880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>30</td>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>32</td>\n",
       "      <td>664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>40</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>36</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>45</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ages  counts\n",
       "26    26    2197\n",
       "0      1    1123\n",
       "28    28     918\n",
       "36    35     880\n",
       "24    24     859\n",
       "25    25     734\n",
       "31    30     724\n",
       "33    32     664\n",
       "27    27     615\n",
       "29    29     570\n",
       "42    40     526\n",
       "37    36     483\n",
       "19     2     482\n",
       "47    45     440\n",
       "23    23     426"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table with the first 15 most dense samples regarding age\n",
    "type(ages), type(counts)\n",
    "s =pd.DataFrame([ages.T, np.array(counts).T],['ages', 'counts'])\n",
    "s=s.transpose()\n",
    "more_dense = s.sort_values(by=['counts'], ascending=False)\n",
    "more_dense.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec47f333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>gender</th>\n",
       "      <th>img_name</th>\n",
       "      <th>pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110225414790.jpg.chip.jpg</td>\n",
       "      <td>[30.0, 38.0, 50.0, 90.0, 109.0, 113.0, 126.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110225417177.jpg.chip.jpg</td>\n",
       "      <td>[72.0, 81.0, 94.0, 96.0, 77.0, 85.0, 90.0, 71....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110224549512.jpg.chip.jpg</td>\n",
       "      <td>[255.0, 253.0, 252.0, 221.0, 144.0, 174.0, 165...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110225402690.jpg.chip.jpg</td>\n",
       "      <td>[62.0, 53.0, 55.0, 62.0, 73.0, 74.0, 86.0, 94....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110225421531.jpg.chip.jpg</td>\n",
       "      <td>[28.0, 60.0, 55.0, 55.0, 74.0, 74.0, 62.0, 101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22037</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110215653284.jpg.chip.jpg</td>\n",
       "      <td>[73.0, 66.0, 78.0, 108.0, 112.0, 124.0, 128.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22038</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110215848132.jpg.chip.jpg</td>\n",
       "      <td>[74.0, 89.0, 51.0, 63.0, 69.0, 80.0, 94.0, 95....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22039</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110220005370.jpg.chip.jpg</td>\n",
       "      <td>[98.0, 90.0, 90.0, 111.0, 133.0, 149.0, 169.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22040</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110220016235.jpg.chip.jpg</td>\n",
       "      <td>[57.0, 74.0, 93.0, 92.0, 110.0, 124.0, 130.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22041</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110215516060.jpg.chip.jpg</td>\n",
       "      <td>[156.0, 161.0, 139.0, 140.0, 103.0, 89.0, 95.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22042 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  ethnicity  gender                        img_name  \\\n",
       "0       10          0       0  20170110225414790.jpg.chip.jpg   \n",
       "1       10          0       0  20170110225417177.jpg.chip.jpg   \n",
       "2       10          0       0  20170110224549512.jpg.chip.jpg   \n",
       "3       10          0       0  20170110225402690.jpg.chip.jpg   \n",
       "4       10          0       0  20170110225421531.jpg.chip.jpg   \n",
       "...    ...        ...     ...                             ...   \n",
       "22037    9          0       0  20170110215653284.jpg.chip.jpg   \n",
       "22038    9          0       0  20170110215848132.jpg.chip.jpg   \n",
       "22039    9          0       0  20170110220005370.jpg.chip.jpg   \n",
       "22040    9          0       0  20170110220016235.jpg.chip.jpg   \n",
       "22041    9          0       0  20170110215516060.jpg.chip.jpg   \n",
       "\n",
       "                                                  pixels  \n",
       "0      [30.0, 38.0, 50.0, 90.0, 109.0, 113.0, 126.0, ...  \n",
       "1      [72.0, 81.0, 94.0, 96.0, 77.0, 85.0, 90.0, 71....  \n",
       "2      [255.0, 253.0, 252.0, 221.0, 144.0, 174.0, 165...  \n",
       "3      [62.0, 53.0, 55.0, 62.0, 73.0, 74.0, 86.0, 94....  \n",
       "4      [28.0, 60.0, 55.0, 55.0, 74.0, 74.0, 62.0, 101...  \n",
       "...                                                  ...  \n",
       "22037  [73.0, 66.0, 78.0, 108.0, 112.0, 124.0, 128.0,...  \n",
       "22038  [74.0, 89.0, 51.0, 63.0, 69.0, 80.0, 94.0, 95....  \n",
       "22039  [98.0, 90.0, 90.0, 111.0, 133.0, 149.0, 169.0,...  \n",
       "22040  [57.0, 74.0, 93.0, 92.0, 110.0, 124.0, 130.0, ...  \n",
       "22041  [156.0, 161.0, 139.0, 140.0, 103.0, 89.0, 95.0...  \n",
       "\n",
       "[22042 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = data.drop(data[data.age==26].index).copy()\n",
    "data = data.drop(data[data.age<2].index).copy()\n",
    "data = data.drop(data[data.age>80].index).copy()\n",
    "\n",
    "data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "737e0c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAVW0lEQVR4nO3df7DldX3f8edLUJOAYaFsd/ixdEndaEhaV7vhR3BaFAIL0wZtLYVmdKOka0eI0DrpQDJTTBw7dsb4I6lSV92KHYWgQtxQRlyQmtEpPxaCsMuKbAXCrgu7CAEbpkbou3+czx0Pu/fu99zde37ce5+PmTPn+/18v+fc973nnPs638/3cz4nVYUkSfvzsnEXIEmafIaFJKmTYSFJ6mRYSJI6GRaSpE6HjruAYTj66KNrxYoV4y5DkuaVe+6556mqWjrdtgUZFitWrGDz5s3jLkOS5pUkj820zW4oSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUqcF+QluzS/ves9l/OCpZ1/SduzRR7Dhkx8fU0WS9mZYaOx+8NSzLDnr3S9tu/VTY6pG0nTshpIkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnYYWFkmWJ7k9yYNJtia5rLW/P8nOJPe1y3l9t7kyyfYkDyU5p699TWvbnuSKYdUsSZreML+D+wXgfVV1b5JXAfck2dS2fbSqPty/c5KTgAuBXwaOBW5N8ott8yeAXwd2AHcn2VhVDw6xdklSn6GFRVXtAna15R8l2QYct5+bnA9cV1U/Bh5Jsh04uW3bXlXfB0hyXdvXsJCkERnJOYskK4DXA3e2pkuT3J9kQ5IjW9txwON9N9vR2mZq3/tnrEuyOcnmPXv2zPWvIEmL2tDDIsnhwFeAy6vqOeBq4O8Dq+gdefzRXPycqlpfVauravXSpUvn4i4lSc0wz1mQ5OX0guILVXUDQFU92bf908BNbXUnsLzv5se3NvbTLkkagWGOhgrwWWBbVX2kr/2Yvt3eCmxpyxuBC5O8MsmJwErgLuBuYGWSE5O8gt5J8I3DqluStK9hHlmcDrwdeCDJfa3t94CLkqwCCngUeDdAVW1Ncj29E9cvAJdU1YsASS4FbgEOATZU1dYh1i1J2sswR0N9C8g0m27ez20+CHxwmvab93c7SdJw+QluSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1GlpYJFme5PYkDybZmuSy1n5Ukk1JHm7XR7b2JPnjJNuT3J/kDX33tbbt/3CStcOqWZI0vWEeWbwAvK+qTgJOBS5JchJwBXBbVa0EbmvrAOcCK9tlHXA19MIFuAo4BTgZuGoqYCRJozG0sKiqXVV1b1v+EbANOA44H7im7XYN8Ja2fD7w+eq5A1iS5BjgHGBTVT1dVc8Am4A1w6pbkrSvkZyzSLICeD1wJ7Csqna1TU8Ay9ryccDjfTfb0dpmapckjcjQwyLJ4cBXgMur6rn+bVVVQM3Rz1mXZHOSzXv27JmLu5QkNUMNiyQvpxcUX6iqG1rzk617iXa9u7XvBJb33fz41jZT+0tU1fqqWl1Vq5cuXTq3v4gkLXLDHA0V4LPAtqr6SN+mjcDUiKa1wFf72t/RRkWdCjzbuqtuAc5OcmQ7sX12a5MkjcihQ7zv04G3Aw8kua+1/R7wIeD6JBcDjwEXtG03A+cB24HngXcCVNXTST4A3N32+8OqenqIdUuS9jK0sKiqbwGZYfOZ0+xfwCUz3NcGYMPcVSdJmg0/wS1J6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqNFBYJDl9kDZJ0sI06JHFnwzYJklagA7d38YkpwG/BixN8u/7Nv08cMgwC5MkTY79hgXwCuDwtt+r+tqfA942rKIkSZNlv2FRVd8Evpnkc1X12IhqkiRNmK4jiymvTLIeWNF/m6p68zCKkiRNlkHD4kvAfwU+A7w4vHIkSZNo0NFQL1TV1VV1V1XdM3XZ3w2SbEiyO8mWvrb3J9mZ5L52Oa9v25VJtid5KMk5fe1rWtv2JFfM+jeUJB20QcPiz5O8J8kxSY6aunTc5nPAmmnaP1pVq9rlZoAkJwEXAr/cbvPJJIckOQT4BHAucBJwUdtXkjRCg3ZDrW3Xv9vXVsAvzHSDqvqLJCsGvP/zgeuq6sfAI0m2Aye3bdur6vsASa5r+z444P1KkubAQGFRVSfO4c+8NMk7gM3A+6rqGeA44I6+fXa0NoDH92o/Zbo7TbIOWAdwwgknzGG5kqSBwqL9c99HVX1+lj/vauAD9I5KPgD8EfCuWd7HtKpqPbAeYPXq1TUX9ylJ6hm0G+pX+5Z/BjgTuBeYVVhU1ZNTy0k+DdzUVncCy/t2Pb61sZ92SdKIDNoN9Tv960mWANfN9oclOaaqdrXVtwJTI6U2Al9M8hHgWGAlcBcQYGWSE+mFxIXAv57tz5UkHZxBjyz29jfAfs9jJLkWOAM4OskO4CrgjCSr6HVDPQq8G6Cqtia5nt6J6xeAS6rqxXY/lwK30JuLakNVbT3AmiVJB2jQcxZ/Tu8fPPT+af8ScP3+blNVF03T/Nn97P9B4IPTtN8M3DxInZKk4Rj0yOLDfcsvAI9V1Y4h1CNJmkADfSivTSj4XXozzx4J/O0wi5IkTZZBvynvAnonnP8lcAFwZxKnKJekRWLQbqjfB361qnYDJFkK3Ap8eViFSZImx6BzQ71sKiiaH87itpKkeW7QI4uvJbkFuLat/yscoSRJi0bXd3C/GlhWVb+b5J8Db2yb/hfwhWEXJ0maDF1HFh8DrgSoqhuAGwCS/IO27Z8NsTZJ0oToOu+wrKoe2Luxta0YSkWSpInTFRZL9rPtZ+ewDknSBOvqhtqc5N9U1af7G5P8NrDfr1XV4vWu91zGD556dp/2Y48+gg2f/PgYKpJ0sLrC4nLgxiS/yU/DYTXwCnqzxkr7+MFTz7LkrHfv237rp8ZQjaS5sN+waN8/8WtJ3gT8Smv+H1X1jaFXJkmaGIN+n8XtwO1DrkWSNKH8FLYkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROA00kKE06v0NDGi7DQguC36EhDdfQuqGSbEiyO8mWvrajkmxK8nC7PrK1J8kfJ9me5P4kb+i7zdq2/8NJ1g6rXknSzIZ5zuJzwJq92q4AbquqlcBtbR3gXGBlu6wDroZeuABXAacAJwNXTQWMJGl0hhYWVfUXwNN7NZ8PXNOWrwHe0tf++eq5A1iS5BjgHGBTVT1dVc8Am9g3gCRJQzbq0VDLqmpXW34CWNaWjwMe79tvR2ubqX0fSdYl2Zxk8549e+a2akla5MY2dLaqCqg5vL/1VbW6qlYvXbp0ru5WksTow+LJ1r1Eu97d2ncCy/v2O761zdQuSRqhUYfFRmBqRNNa4Kt97e9oo6JOBZ5t3VW3AGcnObKd2D67tUmSRmhon7NIci1wBnB0kh30RjV9CLg+ycXAY8AFbfebgfOA7cDzwDsBqurpJB8A7m77/WFV7X3SXJI0ZEMLi6q6aIZNZ06zbwGXzHA/G4ANc1iaJGmW/AS3JtLWLQ+w5oLf2qfd6Tuk8TAsNJF+Uoc4fYc0QZx1VpLUySMLqXHmWmlmhoXUOHOtNDO7oSRJnQwLSVInw0KS1MmwkCR1MiwkSZ0cDSXNIYffaqEyLKYx3QveF/viNd3UIzM9Hxx+q4XKsJjGdC94X+yL13RTj/h80GLjOQtJUiePLDQyM80ku+2h73HaWaOvR9LgDAuNzEwzyf74gcvGUI2k2bAbSpLUybCQJHUyLCRJnTxnoQVtNp+RkDQzw0ILmp+RkOaGYaGB+Kl2aXEzLDQQP9UuLW6e4JYkdfLIQponnNFW42RYSPOEM9pqnAwLzSvOLyWNh2GheWUu5pcycKTZG0tYJHkU+BHwIvBCVa1OchTwp8AK4FHggqp6JkmAjwPnAc8Dv1VV946jbi0MTmgozd44R0O9qapWVdXqtn4FcFtVrQRua+sA5wIr22UdcPXIK5WkRW6Shs6eD1zTlq8B3tLX/vnquQNYkuSYMdQnSYvWuM5ZFPD1JAV8qqrWA8uqalfb/gSwrC0fBzzed9sdrW1XXxtJ1tE78uCEE04YYumSpuPQ3oVtXGHxxqrameTvApuSfLd/Y1VVC5KBtcBZD7B69epZ3VbSwXNo78I2lm6oqtrZrncDNwInA09OdS+1691t953A8r6bH9/aJEkjMvKwSHJYkldNLQNnA1uAjcDattta4KtteSPwjvScCjzb110lSRqBcXRDLQNu7I2I5VDgi1X1tSR3A9cnuRh4DLig7X8zvWGz2+kNnX3n6EuWpMVt5GFRVd8HXjdN+w+BM6dpL+CSEZQ2Ek71LWk+8hPcIzYpU307cmUy+OZB84VhsUg5cmUyTMqbB6mLYaED5hxL0uJhWOiAOcfS/GP3ow6UYSEtIjN1P2762KXTHiUaIppiWEia8ShxpvMn0x2hzLb70aOc+cWwWEAcWaNRme4IZbbdjw6ymF8Mi4M0Se+OHFkzOp7c12JjWBwk3x0tTp7cn2yT9CZuoTAspAVqLs4rjMN0R22z/Sfvm7i5Z1hIC9RcnFcYh+mO2hytNX6GxYDso5bGZ7ajtTT3DIsB2UctaTEzLBY4j4h0MHz+aIphscB5RKSD4fNHU8bytaqSpPnFI4sJ5lhxSZPCsJhgjhWXhs83ZYMxLKR5zpPQB2e2b8oWa7gYFhPAF7sOhiehR2tSjvhHHVqGxQTwxS4tLnPxj37UoWVYDIlHC+rn80H9JuXoZDYMiyHxaEH9fD5ovjMspAnjUcjg/FuNjmGhl/DFN34L7ShkmM+pSfpbTfd7PrL9IU589Wv22Xem332mv9UkjLQyLOahxfLi08IwSc+p6V47c/VGaLrf89kHLpvV7z7Js+saFvPQJL34pPlkutfOfHjdDDPkBmVYSNI0JqlLdhJCbt6ERZI1wMeBQ4DPVNWHxlySpAXMI/iXmhezziY5BPgEcC5wEnBRkpPGW5UkLR7zIiyAk4HtVfX9qvpb4Drg/DHXJEmLRqpq3DV0SvI2YE1V/XZbfztwSlVd2rfPOmBdW30N8NB+7vJo4KkhlXuwrO3AWNuBsbYDs1Br+3tVtXS6DfPmnEWXqloPrB9k3ySbq2r1kEs6INZ2YKztwFjbgVmMtc2XbqidwPK+9eNbmyRpBOZLWNwNrExyYpJXABcCG8dckyQtGvOiG6qqXkhyKXALvaGzG6pq60Hc5UDdVWNibQfG2g6MtR2YRVfbvDjBLUkar/nSDSVJGiPDQpLUaVGFRZI1SR5Ksj3JFRNQz4Yku5Ns6Ws7KsmmJA+36yPHUNfyJLcneTDJ1iSXTVBtP5PkriTfabX9QWs/Mcmd7bH90zYQYiySHJLkL5PcNEm1JXk0yQNJ7kuyubWN/TFtdSxJ8uUk302yLclpE1Tba9rfbOryXJLLJ6G+JP+uvQ62JLm2vT6G8nxbNGExoVOGfA5Ys1fbFcBtVbUSuK2tj9oLwPuq6iTgVOCS9reahNp+DLy5ql4HrALWJDkV+M/AR6vq1cAzwMVjqG3KZcC2vvVJqu1NVbWqbxz+JDym0Jv37WtV9VrgdfT+fhNRW1U91P5mq4B/BDwP3Dju+pIcB7wXWF1Vv0Jv8M+FDOv5VlWL4gKcBtzSt34lcOUE1LUC2NK3/hBwTFs+BnhoAmr8KvDrk1Yb8HPAvcAp9D6xeuh0j/WIazqe3j+ONwM3AZmg2h4Fjt6rbeyPKXAE8AhtwM0k1TZNrWcD356E+oDjgMeBo+iNbL0JOGdYz7dFc2TBT/+wU3a0tkmzrKp2teUngGXjLCbJCuD1wJ1MSG2tm+c+YDewCfjfwF9X1Qttl3E+th8D/gPw/9r632Fyaivg60nuadPjwGQ8picCe4D/1rrvPpPksAmpbW8XAte25bHWV1U7gQ8DfwXsAp4F7mFIz7fFFBbzTvXeGoxtbHOSw4GvAJdX1XP928ZZW1W9WL0ugePpTTL52nHUsbck/xTYXVX3jLuWGbyxqt5Aryv2kiT/uH/jGB/TQ4E3AFdX1euBv2GvLp1xvxYAWt//bwBf2nvbOOpr50jOpxe2xwKHsW+39pxZTGExX6YMeTLJMQDtevc4ikjycnpB8YWqumGSaptSVX8N3E7vUHtJkqkPmY7rsT0d+I0kj9KbGfnN9PriJ6G2qXeiVNVuen3uJzMZj+kOYEdV3dnWv0wvPCahtn7nAvdW1ZNtfdz1nQU8UlV7quonwA30noNDeb4tprCYL1OGbATWtuW19M4XjFSSAJ8FtlXVRyastqVJlrTln6V3LmUbvdB42zhrq6orq+r4qlpB7/n1jar6zUmoLclhSV41tUyv730LE/CYVtUTwONJXtOazgQenITa9nIRP+2CgvHX91fAqUl+rr1mp/5uw3m+jfuE0YhPCJ0HfI9eH/fvT0A919Lra/wJvXdXF9Pr474NeBi4FThqDHW9kd4h9f3Afe1y3oTU9g+Bv2y1bQH+Y2v/BeAuYDu9boJXjvmxPQO4aVJqazV8p122Tj3/J+ExbXWsAja3x/XPgCMnpbZW32HAD4Ej+trGXh/wB8B322vhvwOvHNbzzek+JEmdFlM3lCTpABkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSHNsSR/1ibr2zo1YV+Si5N8r30Xx6eT/JfWvjTJV5Lc3S6nj7d6aXp+KE+aY0mOqqqn23Qkd9ObNvrb9OY7+hHwDeA7VXVpki8Cn6yqbyU5gd500r80tuKlGRzavYukWXpvkre25eXA24FvVtXTAEm+BPxi234WcFJvah8Afj7J4VX1f0ZZsNTFsJDmUJIz6AXAaVX1fJL/SW/unpmOFl4GnFpV/3ckBUoHyHMW0tw6AnimBcVr6X0t7WHAP0lyZJs6+l/07f914HemVpKsGmWx0qAMC2lufQ04NMk24EPAHfS+T+A/0ZsJ9Nv0vt702bb/e4HVSe5P8iDwb0desTQAT3BLIzB1HqIdWdwIbKiqG8ddlzQojyyk0Xh/+97wLcAj9L6zQZo3PLKQJHXyyEKS1MmwkCR1MiwkSZ0MC0lSJ8NCktTp/wNfKX6dBSn3zgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(data.age);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9bc38c",
   "metadata": {},
   "source": [
    "mirroring the images of ages les dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7508173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "\n",
    "def pixel_mirroring(pixel):\n",
    "    '''Mirroring a pixel from the prepared dataset'''\n",
    "\n",
    "    #Turn the pixel list into an array 48,48\n",
    "    array=np.uint8(pixel.reshape((48,48)))\n",
    "\n",
    "    #Turn the array into an image object and mirror it\n",
    "    img=Image.fromarray(array, 'L')\n",
    "    img=ImageOps.mirror(img)\n",
    "\n",
    "    #Turn the image back into a pixel list\n",
    "    img=img.getdata()\n",
    "    pixel_mirror=np.array(img, dtype=np.float32)\n",
    "\n",
    "    return pixel_mirror\n",
    "\n",
    "def count_amout_per_age(data):\n",
    "    ages = data['age'].unique()\n",
    "    counts = []\n",
    "    for age in ages:\n",
    "        counts.append(np.count_nonzero(data['age']==age))\n",
    "\n",
    "    type(ages), type(counts)\n",
    "    s =pd.DataFrame([ages.T, np.array(counts).T],['ages', 'counts'])\n",
    "    s=s.transpose()\n",
    "    more_dense = s.sort_values(by=['counts'], ascending=True)\n",
    "    more_dense.head(15)\n",
    "    return more_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d31bdd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = count_amout_per_age(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19600db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "more = dense[dense.counts>600].copy()['ages'].unique()\n",
    "less = dense[dense.counts<600].copy()['ages'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4738af64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_less = data.copy()\n",
    "data_more = data.copy()\n",
    "\n",
    "for age in more:\n",
    "    data_less = data_less.drop(data_less[data_less.age==age].index).copy()\n",
    "\n",
    "for age in less:\n",
    "    data_more = data_more.drop(data_more[data_more.age==age].index).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ba69780",
   "metadata": {},
   "outputs": [],
   "source": [
    "mirrored = []\n",
    "for images in data_less['pixels']:\n",
    "    mirrored.append(pixel_mirroring(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db137ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_less_mirrored = data_less.copy()\n",
    "data_less_mirrored['mirrored']=mirrored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ead5feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_less_mirrored.drop(columns=['pixels'], inplace=True)\n",
    "data_less_mirrored.rename(columns={'mirrored':'pixels'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8ed1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two data frames, reste indexes\n",
    "presque = pd.concat([data_less, data_less_mirrored, data_more]).reset_index(drop=True, inplace=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2e2dbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUOElEQVR4nO3df7DldX3f8ecLiSbBBJbu7c4GsEuajYbainbDj+i0RBKEHRu0tRQbdetAtqQQtM2khaRTomk6dMaaSJLCENwKVSGoEAmzI65ozMSpyEIQWRZkqxCWWdhdIasN00Tsu3+c7y2ne+/d792795zv99zzfMycOd/v5/s95773nnP3db6fz/f7OakqJEk6lKO6LkCS1H+GhSSplWEhSWplWEiSWhkWkqRWR3ddwCisXr261q1b13UZkjRR7rvvvv1VNTPfthUZFuvWrWP79u1dlyFJEyXJEwttsxtKktTKsJAktTIsJEmtDAtJUivDQpLUyrCQJLUyLCRJrQwLSVIrw0KS1GpFXsGtyfGOiy9hz/4Dc9rXrj6Wj95wXQcVSZqPYaFO7dl/gJmNl89t33pNB9VIWojdUJKkVoaFJKmVYSFJamVYSJJaGRaSpFaGhSSplWEhSWplWEiSWhkWkqRWhoUkqZVhIUlqZVhIkloZFpKkViMLiyQnJflCkoeT7Ejynqb9+CTbkjzW3K9q2pPkmiS7kjyY5HVDz7Wp2f+xJJtGVbMkaX6jPLJ4AfjlqjoFOAO4NMkpwBXA3VW1Hri7WQc4D1jf3DYD18IgXICrgNOB04CrZgNGkjQeIwuLqtpTVfc3y98BdgInAOcDNza73Qi8pVk+H7ipBr4MHJdkLfAmYFtVPVtVzwHbgHNHVbckaa6xjFkkWQe8FrgHWFNVe5pNTwNrmuUTgCeHHra7aVuo/eCfsTnJ9iTb9+3bt7z/AEmaciMPiyQvBz4FvLeqvj28raoKqOX4OVV1fVVtqKoNMzMzy/GUkqTGSMMiyfcxCIqPVdVtTfMzTfcSzf3epv0p4KShh5/YtC3ULkkak1GeDRXgw8DOqvrg0KY7gNkzmjYBnx5qf1dzVtQZwIGmu+ou4Jwkq5qB7XOaNknSmBw9wud+PfBO4GtJHmjafhW4Grg1yUXAE8AFzbatwEZgF/A88G6Aqno2yW8A9zb7vb+qnh1h3ZKkg4wsLKrqT4EssPnsefYv4NIFnmsLsGX5qpMkHQ6v4JYktTIsJEmtDAtJUivDQpLUyrCQJLUyLCRJrQwLSVKrUV6UJy27d1x8CXv2H5jTvnb1sXz0hus6qEiaDoaFJsqe/QeY2Xj53Pat13RQjTQ9DAvpIB69SHMZFtJBPHqR5nKAW5LUyrCQJLUyLCRJrQwLSVIrw0KS1MqwkCS1MiwkSa0MC0lSK8NCktTKsJAktTIsJEmtDAtJUivDQpLUyrCQJLUyLCRJrQwLSVIrw0KS1MqwkCS1MiwkSa0MC0lSK8NCktTKsJAktTq66wI0Hd5x8SXs2X9gTvsjX3+MmY0dFCTpsBgWGos9+w8ws/HyOe0P7vjFDqqRdLjshpIktTIsJEmt7Iaax0L962tXH8tHb7iug4okqVsjC4skW4A3A3ur6tVN268DvwDsa3b71ara2my7ErgI+B5weVXd1bSfC3wIeAlwQ1VdPaqaZy3Uv75n6zWj/tFq7Hx4B2e/5e1z2h0Ql7oxyiOLjwC/C9x0UPtvVdUHhhuSnAJcCPwd4EeAzyX58Wbz7wE/C+wG7k1yR1U9PMK61QPfraMcEJd6ZGRhUVV/kmTdInc/H7ilqv4K+GaSXcBpzbZdVfUNgCS3NPsaFpI0Rl0McF+W5MEkW5KsatpOAJ4c2md307ZQ+xxJNifZnmT7vn375ttFkrRE4w6La4G/DZwK7AH+y3I9cVVdX1UbqmrDzMzMcj2tJIkxnw1VVc/MLif5feDOZvUp4KShXU9s2jhEuzrkGWPSdBlrWCRZW1V7mtW3Ag81y3cAH0/yQQYD3OuBrwAB1ic5mUFIXAj883HWrPl5xpg0XUZ56uzNwFnA6iS7gauAs5KcChTwOPAvAapqR5JbGQxcvwBcWlXfa57nMuAuBqfObqmqHaOqWZI0v1GeDTX3JHn48CH2/03gN+dp3wpsXcbSJEmHyek+JEmtDAtJUivDQpLUyrCQJLUyLCRJrQwLSVIrw0KS1MqwkCS1MiwkSa0MC0lSK8NCktTKsJAktTIsJEmtDAtJUivDQpLUalFhkeT1i2mTJK1Miz2y+J1FtkmSVqBDflNekjOBnwJmkvyboU0/zOBrTiVJU6Dta1VfCry82e+Hhtq/DbxtVEVJkvrlkGFRVV8EvpjkI1X1xJhqktRz77j4EvbsPzCnfe3qY/noDdd1UJFGre3IYtbLklwPrBt+TFW9cRRFSeq3PfsPMLPx8rntW6/poBqNw2LD4hPAdcANwPdGV44kqY8WGxYvVNW1I61EktRbiz119o+S/Kska5McP3sbaWWSpN5Y7JHFpub+V4baCvjR5S1HktRHiwqLqjp51IVIk8ozgzQNFhUWSd41X3tV3bS85UiTxzODNA0W2w31k0PL3w+cDdwPGBaSNAUW2w31S8PrSY4DbhlFQZKk/lnqFOV/CTiOIUlTYrFjFn/E4OwnGEwg+BPAraMqSpLUL4sds/jA0PILwBNVtXsE9UiSemhR3VDNhIKPMJh5dhXw16MsSpLUL4v9prwLgK8A/xS4ALgniVOUS9KUWGw31K8BP1lVewGSzACfAz45qsIkSf2x2LOhjpoNisa3DuOxkqQJt9gji88kuQu4uVn/Z8DW0ZQkaRSclkRHou07uH8MWFNVv5LkHwNvaDb9D+Bjoy5OmmQ7H97B2W95+5z2rv5zdloSHYm2I4vfBq4EqKrbgNsAkvzdZts/WuiBSbYAbwb2VtWrm7bjgT9g8I17jwMXVNVzSQJ8CNgIPA/8i6q6v3nMJuDfN0/7H6vqxsP8N0qd+G4d5X/OWjHaxh3WVNXXDm5s2ta1PPYjwLkHtV0B3F1V64G7m3WA84D1zW0zcC38v3C5CjgdOA24Ksmqlp8rSVpmbWFx3CG2/cChHlhVfwI8e1Dz+cDskcGNwFuG2m+qgS8DxyVZC7wJ2FZVz1bVc8A25gaQJGnE2sJie5JfOLgxycXAfUv4eWuqak+z/DSwplk+AXhyaL/dTdtC7ZKkMWobs3gvcHuSn+fFcNgAvBR465H84KqqJNW+5+Ik2cygC4tXvOIVy/W0kiRajiyq6pmq+ingfQwGpB8H3ldVZ1bV00v4ec803Us097PXbjwFnDS034lN20Lt89V6fVVtqKoNMzMzSyhNkrSQxc4N9YWq+p3m9vkj+Hl38OL3eW8CPj3U/q4MnAEcaLqr7gLOSbKqGdg+p2mTJI3RYi/KO2xJbgbOAlYn2c3grKargVuTXAQ8wWCeKRhc4LcR2MXg1Nl3A1TVs0l+A7i32e/9VXXwoLkkacRGFhZVNfdqpIGz59m3gEsXeJ4twJZlLE1TxKuWpeUxsrBYifp2Ra7aedXy0i0UtACPfP0xZjaOuSB1yrA4DF6Rq2myUNACPLjjF8dcjbrmzLGSpFaGhSSpld1QknrHExP6x7CQ1DuemNA/hoXUc37KVh8YFlLP+SlbfWBYaFktdC2K5+VLk82wWIG67LZY6FoUz8uXJpthsQLZbSFpuXmdhSSplUcWmkoLja2A4yvSfAyLEfKUx/5aaGwFHF+R5mNYjJBjB5qPsxdrEhkW0pg5e7EmkQPckqRWhoUkqZVhIUlq5ZiFpJHzzMDJZ1hIGjnPDJx8hoUAP/lJOjTDQoCf/CQdmmGhFcGp0aXRMiy0Ijg1ujRanjorSWplWEiSWhkWkqRWjllIE8rZazVOhoU0oZy9VuNkN5QkqZVhIUlqZVhIkloZFpKkVg5wS4vklCKaZoaFDsn/IF/klCKaZoaFDsn/ICWBYxaSpEUwLCRJrToJiySPJ/lakgeSbG/ajk+yLcljzf2qpj1JrkmyK8mDSV7XRc2SNM26HLP46araP7R+BXB3VV2d5Ipm/d8B5wHrm9vpwLXNvbSieDLB8vPrgpdPnwa4zwfOapZvBP6YQVicD9xUVQV8OclxSdZW1Z5OqpRGxJMJlp9fF7x8uhqzKOCzSe5LsrlpWzMUAE8Da5rlE4Anhx67u2n7/yTZnGR7ku379u0bVd2SNJW6OrJ4Q1U9leRvAtuSPDK8saoqSR3OE1bV9cD1ABs2bDisx0qSDq2TsKiqp5r7vUluB04DnpntXkqyFtjb7P4UcNLQw09s2qbeQv2x9nGrK467rFxjD4skxwBHVdV3muVzgPcDdwCbgKub+083D7kDuCzJLQwGtg84XjGwUH+sfdzqiuMuK1cXRxZrgNuTzP78j1fVZ5LcC9ya5CLgCeCCZv+twEZgF/A88O7xlyxJ023sYVFV3wBeM0/7t4Cz52kv4NIxlNY5T/OT1Fd9OnV26nman6S+croPSVIrw0KS1MqwkCS1MiwkSa0MC0lSK8NCktTKU2claYwm9Xoqw0KSxmhSr6cyLCRNnYUmPOz7p/suGRYdcGZOqVsLTXjY90/3XTIsOtDVzJwLhRQYVBJM1hHHuMc+DIspslBIgVNIryTTeOS6XN/tMklHHOMe+zAspBVmGr9TYtTf7XKoo/I+HnWMgmEhSS0OdVTex6OOUTAspCk3jd1WOnyGxTIY9R+bf8wapWnstlpOkzQofiQMi2Uw6j82/5ilgT5+cJqkQfEjYVhImhh+cOqOEwlKkloZFpKkVoaFJKmVYSFJauUAtyT12HJNZXKkDAtJ6rFRT2WyWIaFpM708boJzc+wkNQZr5uYHA5wS5JaGRaSpFZ2Q0nSCKy08RjDQpJG4HDHY/oeLoaFJPVA3wf7HbOQJLUyLCRJrQwLSVIrw0KS1MqwkCS1MiwkSa0mJiySnJvk0SS7klzRdT2SNE0mIiySvAT4PeA84BTg7UlO6bYqSZoeExEWwGnArqr6RlX9NXALcH7HNUnS1EhVdV1DqyRvA86tqoub9XcCp1fVZUP7bAY2N6uvBB49xFOuBvaPqNwjZW1LY21LY21Ls1Jr+1tVNTPfhhUz3UdVXQ9cv5h9k2yvqg0jLmlJrG1prG1prG1pprG2SemGego4aWj9xKZNkjQGkxIW9wLrk5yc5KXAhcAdHdckSVNjIrqhquqFJJcBdwEvAbZU1Y4jeMpFdVd1xNqWxtqWxtqWZupqm4gBbklStyalG0qS1CHDQpLUaqrCom9ThiTZkmRvkoeG2o5Psi3JY839qg7qOinJF5I8nGRHkvf0qLbvT/KVJF9tantf035yknua1/YPmhMhOpHkJUn+LMmdfaotyeNJvpbkgSTbm7bOX9OmjuOSfDLJI0l2JjmzR7W9svmdzd6+neS9fagvyb9u/g4eSnJz8/cxkvfb1IRFT6cM+Qhw7kFtVwB3V9V64O5mfdxeAH65qk4BzgAubX5Xfajtr4A3VtVrgFOBc5OcAfxn4Leq6seA54CLOqht1nuAnUPrfartp6vq1KHz8PvwmgJ8CPhMVb0KeA2D318vaquqR5vf2anA3weeB27vur4kJwCXAxuq6tUMTv65kFG936pqKm7AmcBdQ+tXAlf2oK51wEND648Ca5vltcCjPajx08DP9q024AeB+4HTGVyxevR8r/WYazqRwX8cbwTuBNKj2h4HVh/U1vlrChwLfJPmhJs+1TZPrecAX+pDfcAJwJPA8QzObL0TeNOo3m9Tc2TBi7/YWbubtr5ZU1V7muWngTVdFpNkHfBa4B56UlvTzfMAsBfYBvxP4C+q6oVmly5f298G/i3wf5r1v0F/aivgs0nua6bHgX68picD+4D/1nTf3ZDkmJ7UdrALgZub5U7rq6qngA8Afw7sAQ4A9zGi99s0hcXEqcFHg87ObU7ycuBTwHur6tvD27qsraq+V4MugRMZTDL5qi7qOFiSNwN7q+q+rmtZwBuq6nUMumIvTfIPhjd2+JoeDbwOuLaqXgv8JQd16XT9twDQ9P3/HPCJg7d1UV8zRnI+g7D9EeAY5nZrL5tpCotJmTLkmSRrAZr7vV0UkeT7GATFx6rqtj7VNquq/gL4AoND7eOSzF5k2tVr+3rg55I8zmBm5Dcy6IvvQ22zn0Spqr0M+txPox+v6W5gd1Xd06x/kkF49KG2YecB91fVM8161/X9DPDNqtpXVd8FbmPwHhzJ+22awmJSpgy5A9jULG9iMF4wVkkCfBjYWVUf7FltM0mOa5Z/gMFYyk4GofG2Lmurqiur6sSqWsfg/fX5qvr5PtSW5JgkPzS7zKDv/SF68JpW1dPAk0le2TSdDTzch9oO8nZe7IKC7uv7c+CMJD/Y/M3O/t5G837resBozANCG4GvM+jj/rUe1HMzg77G7zL4dHURgz7uu4HHgM8Bx3dQ1xsYHFI/CDzQ3Db2pLa/B/xZU9tDwH9o2n8U+Aqwi0E3wcs6fm3PAu7sS21NDV9tbjtm3/99eE2bOk4Ftjev6x8Cq/pSW1PfMcC3gGOH2jqvD3gf8Ejzt/DfgZeN6v3mdB+SpFbT1A0lSVoiw0KS1MqwkCS1MiwkSa0MC0lSK8NCktTKsJAktTIspGWW5A+byfp2zE7Yl+SiJF9vvovj95P8btM+k+RTSe5tbq/vtnppfl6UJy2zJMdX1bPNdCT3Mpg2+ksM5jv6DvB54KtVdVmSjwP/tar+NMkrGEwn/ROdFS8t4Oj2XSQdpsuTvLVZPgl4J/DFqnoWIMkngB9vtv8McMpgah8AfjjJy6vqf42zYKmNYSEtoyRnMQiAM6vq+SR/zGDunoWOFo4Czqiq/z2WAqUlcsxCWl7HAs81QfEqBl9LewzwD5OsaqaO/idD+38W+KXZlSSnjrNYabEMC2l5fQY4OslO4Grgywy+T+A/MZgJ9EsMvt70QLP/5cCGJA8meRi4ZOwVS4vgALc0BrPjEM2Rxe3Alqq6veu6pMXyyEIaj19vvjf8IeCbDL6zQZoYHllIklp5ZCFJamVYSJJaGRaSpFaGhSSplWEhSWr1fwExKeZM0g8WgQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(presque['age']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c59d0049",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = presque.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ca3a4",
   "metadata": {},
   "source": [
    "Steping and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3fe75b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_size = 15\n",
    "#input_list = data['age']\n",
    "#cat = age_categorize(input_list, step_size)  # ====>>  INITIALIZE THIS FUNCTION  <<=========\n",
    "\n",
    "input_list = data['age']\n",
    "custom = [0, 15, 35, 60, 150]\n",
    "cat = age_categorize_custom(input_list, custom)  # ====>>  INITIALIZE THIS FUNCTION  <<=========\n",
    "\n",
    "\n",
    "#pd.DataFrame(cat, data['age'].values).sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "affd3a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add categorical age clasification to original dataframe\n",
    "data['points_bin']=cat\n",
    "#data[['age','class_age']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92570744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(data['points_bin']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "716e7eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### perform one-hot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse = False)\n",
    "ohe.fit(data[['points_bin']])\n",
    "class_age_encoded = ohe.transform(data[['points_bin']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "662905ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elements in range(class_age_encoded.shape[1]):      # =====> THIS IS NEED WHATHERVER HOT ENCODER USED  <=====\n",
    "    data[str(elements)]=class_age_encoded[:,elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4f185f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>gender</th>\n",
       "      <th>img_name</th>\n",
       "      <th>pixels</th>\n",
       "      <th>points_bin</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26422</th>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>20170117174145046.jpg.chip.jpg</td>\n",
       "      <td>[171.0, 170.0, 163.0, 155.0, 151.0, 145.0, 130...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32978</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20170117180022765.jpg.chip.jpg</td>\n",
       "      <td>[165.0, 166.0, 167.0, 129.0, 87.0, 59.0, 56.0,...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35950</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170117152434773.jpg.chip.jpg</td>\n",
       "      <td>[75.0, 76.0, 73.0, 71.0, 69.0, 68.0, 70.0, 48....</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31447</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20170113155100771.jpg.chip.jpg</td>\n",
       "      <td>[137.0, 158.0, 177.0, 186.0, 192.0, 192.0, 191...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12848</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20170117143355801.jpg.chip.jpg</td>\n",
       "      <td>[170.0, 170.0, 166.0, 172.0, 178.0, 165.0, 138...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5985</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20170117191953052.jpg.chip.jpg</td>\n",
       "      <td>[159.0, 160.0, 162.0, 163.0, 164.0, 167.0, 168...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23521</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20170109220855652.jpg.chip.jpg</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 223.0, 207...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22603</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170117172231908.jpg.chip.jpg</td>\n",
       "      <td>[23.0, 5.0, 7.0, 10.0, 34.0, 35.0, 38.0, 31.0,...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20337</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170117171523531.jpg.chip.jpg</td>\n",
       "      <td>[50.0, 49.0, 70.0, 93.0, 120.0, 132.0, 136.0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22325</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170117140802794.jpg.chip.jpg</td>\n",
       "      <td>[115.0, 114.0, 81.0, 67.0, 49.0, 66.0, 83.0, 9...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  ethnicity  gender                        img_name  \\\n",
       "26422   60          3       0  20170117174145046.jpg.chip.jpg   \n",
       "32978   27          0       1  20170117180022765.jpg.chip.jpg   \n",
       "35950   35          0       0  20170117152434773.jpg.chip.jpg   \n",
       "31447   26          1       0  20170113155100771.jpg.chip.jpg   \n",
       "12848   67          1       0  20170117143355801.jpg.chip.jpg   \n",
       "5985    37          1       0  20170117191953052.jpg.chip.jpg   \n",
       "23521   49          0       1  20170109220855652.jpg.chip.jpg   \n",
       "22603   45          0       0  20170117172231908.jpg.chip.jpg   \n",
       "20337   37          0       0  20170117171523531.jpg.chip.jpg   \n",
       "22325   43          0       0  20170117140802794.jpg.chip.jpg   \n",
       "\n",
       "                                                  pixels  points_bin    0  \\\n",
       "26422  [171.0, 170.0, 163.0, 155.0, 151.0, 145.0, 130...           4  0.0   \n",
       "32978  [165.0, 166.0, 167.0, 129.0, 87.0, 59.0, 56.0,...           2  0.0   \n",
       "35950  [75.0, 76.0, 73.0, 71.0, 69.0, 68.0, 70.0, 48....           3  0.0   \n",
       "31447  [137.0, 158.0, 177.0, 186.0, 192.0, 192.0, 191...           2  0.0   \n",
       "12848  [170.0, 170.0, 166.0, 172.0, 178.0, 165.0, 138...           4  0.0   \n",
       "5985   [159.0, 160.0, 162.0, 163.0, 164.0, 167.0, 168...           3  0.0   \n",
       "23521  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 223.0, 207...           3  0.0   \n",
       "22603  [23.0, 5.0, 7.0, 10.0, 34.0, 35.0, 38.0, 31.0,...           3  0.0   \n",
       "20337  [50.0, 49.0, 70.0, 93.0, 120.0, 132.0, 136.0, ...           3  0.0   \n",
       "22325  [115.0, 114.0, 81.0, 67.0, 49.0, 66.0, 83.0, 9...           3  0.0   \n",
       "\n",
       "         1    2    3  \n",
       "26422  0.0  0.0  1.0  \n",
       "32978  1.0  0.0  0.0  \n",
       "35950  0.0  1.0  0.0  \n",
       "31447  1.0  0.0  0.0  \n",
       "12848  0.0  0.0  1.0  \n",
       "5985   0.0  1.0  0.0  \n",
       "23521  0.0  1.0  0.0  \n",
       "22603  0.0  1.0  0.0  \n",
       "20337  0.0  1.0  0.0  \n",
       "22325  0.0  1.0  0.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c5865",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## If using piere fiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc70d9e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pierre distribution\n",
    "data_clean = data.copy()\n",
    "data['points_bin'] = pd.qcut(data_clean['age'], q=3)\n",
    "\n",
    "#view updated DataFrame\n",
    "print(data)\n",
    "data['points_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f765eb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# perform one-hot encoder to the Pierre distribution\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse = False)\n",
    "ohe.fit(data[['points_bin']])\n",
    "class_age_encoded = ohe.transform(data[['points_bin']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4439651f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for elements in range(class_age_encoded.shape[1]):\n",
    "    data[str(elements)]=class_age_encoded[:,elements]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d84f9",
   "metadata": {},
   "source": [
    "## categorical fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddfe80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['pixels'].tolist()\n",
    "X = np.reshape(X, (-1, 48, 48,1))\n",
    "\n",
    "y=data.drop(columns=['age','ethnicity','gender', 'pixels', 'points_bin', 'img_name'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa903518",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bece510",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cat = initialize_model_catgorical(X.shape[-1], y.shape[-1])\n",
    "    \n",
    "es = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "model_cat.compile(optimizer='adam' ,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history_cat = model_cat.fit(X_train,y_train, validation_split=0.3, epochs=50, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_cat.history['loss'])\n",
    "plt.plot(history_cat.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddd52ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.save_model(model_cat,'Model_cat_15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a29bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cat.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d111701e",
   "metadata": {},
   "source": [
    "# calling once the categorical branch model is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc5e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = data['points_bin'].unique()[1]\n",
    "upper = data['points_bin'].unique()[2]\n",
    "\n",
    "cacho = data.drop(data[data.points_bin<lower].index | data[data.points_bin>=upper].index).copy()\n",
    "cacho['age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d72e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping for all linear model\n",
    "Histories = []\n",
    "\n",
    "y = data['age']\n",
    "\n",
    "for i in range(len(data['points_bin'].unique())):\n",
    "\n",
    "\n",
    "    # slice the dataframe\n",
    "    \n",
    "    #lower = data['points_bin'].unique()[i].left  # ====>>  USING PIERRE FORMAT  <<=========\n",
    "    #upper = data['points_bin'].unique()[i].right\n",
    "    #cacho = data.drop(data[data.age<lower].index | data[data.age>=upper].index).copy()\n",
    "    \n",
    "    lower = data['points_bin'].unique()[i]   # ====>>  USING JAVIER FORMAT  <<=========\n",
    "    upper = data['points_bin'].unique()[i+1]\n",
    "    cacho = data.drop(data[data.points_bin<lower].index | data[data.points_bin>=upper].index).copy()\n",
    "\n",
    "    \n",
    "    # prepare the data\n",
    "    X = cacho['pixels'].tolist()\n",
    "    X = np.reshape(X, (-1, 48, 48,1))\n",
    "\n",
    "    y=cacho['age']\n",
    "\n",
    "    # split data set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.3, random_state=1)\n",
    "    \n",
    "    \n",
    "    print(' ')\n",
    "    print('*****************************************************************************************')\n",
    "    print(f'STARTING MODEL =======>>>>>> {i} with age range {y.min()} to {y.max()} and {X.shape} samples')\n",
    "    print('*****************************************************************************************')\n",
    "    print(' ')\n",
    "\n",
    "\n",
    "    # initialize the model\n",
    "    model = initialize_model_regression()\n",
    "        \n",
    "    # compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    # early stopping\n",
    "    es = EarlyStopping(monitor='mae', patience=6, restore_best_weights=True)\n",
    "\n",
    "\n",
    "    # fit\n",
    "    history = model.fit(X_train, y_train, validation_split=0.3, epochs=40, callbacks=[es])\n",
    "    \n",
    "    # save model\n",
    "    Histories.append(history)\n",
    "    \n",
    "    models.save_model(model, f'Model_linear_{y.min()}_{y.max()}')\n",
    "    \n",
    "    # delete variables to save RAM\n",
    "    del model, X, y, X_train, X_test, y_train, y_test, es, history, cacho\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d3a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870e4f17",
   "metadata": {},
   "source": [
    "## Evaluation test categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b247360",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['pixels'].tolist()\n",
    "X = np.reshape(X, (-1, 48, 48,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33abb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=107\n",
    "plt.imshow(X[n], cmap='gray');\n",
    "#np.where(y.iloc[n]==1)[0]\n",
    "print(f\"real age is {data.iloc[n]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2086f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_inp = np.expand_dims(X[n], axis=0)\n",
    "model_cat.predict(try_inp).max()\n",
    "index = np.where(model_cat.predict(try_inp)==(model_cat.predict(try_inp).max()))\n",
    "print(f'slot number {index[1][0]}, correspond to range {(index[1][0]+1)*step_size-step_size} to {(index[1][0]+1)*step_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22747822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_cat.predict(try_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8bf66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_inp = np.expand_dims(X[n], axis=0)\n",
    "# model_cat.predict(try_inp).max()\n",
    "# index = np.where(model_cat.predict(try_inp)==(model_cat.predict(try_inp).max()))\n",
    "# index[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8370a375",
   "metadata": {},
   "source": [
    "## Evaluation test regresional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94327e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c4ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corresponding regresional model\n",
    "predict_model = models.load_model('Model_linear_1_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f0f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict regresional\n",
    "predict_model.predict(try_inp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
