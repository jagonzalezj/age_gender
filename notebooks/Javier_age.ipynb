{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78103890",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942f409",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b19dd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#data_path = '/home/gonzalez/Desktop/age_gender/age_gender.csv' # old data\n",
    "# url = 'https://www.kaggle.com/code/shahraizanwar/age-gender-ethnicity-prediction/data?select=age_gender.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c219bbff",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# processing color data from Pierre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61de7b9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    " # new data from Pierre\n",
    "data_path = '/home/gonzalez/code/jagonzalezj/age_gender/raw_data/new_data.csv' \n",
    "data = pd.read_csv(data_path, encoding='utf-8')\n",
    "\n",
    "lola =[]   \n",
    "for i in range(len(data['image'])):\n",
    "    a = data['image'][i].replace('[',',').replace(']',',').replace(',','').split()\n",
    "    lola.append([int(j) for j in a])\n",
    "    \n",
    "data['image']=lola\n",
    "data.columns=['age', 'gender', 'ethnicity', 'pixels']\n",
    "\n",
    "data['ethnicity'].unique()[5:]\n",
    "for items in data['ethnicity'].unique()[5:]:\n",
    "    data = data.drop(data[data.ethnicity==items].index).copy()\n",
    "\n",
    "data = data.dropna()\n",
    "data.reindex()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec1061",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Loading data from google cloud to google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612cad54",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive')\n",
    "df=pd.read_csv('gdrive/My Drive/age_gender.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de094b9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Transforming the pixels data type into a list of float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67406a11",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# images =[]\n",
    "# for fotos in range(len(data['pixels'])):\n",
    "#     X = data['pixels'][fotos].split(\" \")\n",
    "#     X = list(map(int, X))\n",
    "#     images.append(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c382e670",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# x = np.reshape(images[5000], (48,48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2034415d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data['pixels']=data['pixels'].apply(lambda x:  np.array(x.split(), dtype=\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1367365",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "blob = data['pixels'][0].reshape(48,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c74751",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(blob, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a953707",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#sns.displot(data['ethnicity']),\n",
    "#sns.displot(data['gender']), \n",
    "#sns.displot(data['age']);\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 5))\n",
    "sns.histplot(ax=axes[0], x=data['age']);\n",
    "sns.histplot(ax=axes[1], x=data['ethnicity']);\n",
    "sns.histplot(ax=axes[2], x=data['gender']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3251d861",
   "metadata": {},
   "source": [
    "# Working with the age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the number of counts per age\n",
    "ages = data['age'].unique()\n",
    "counts = []\n",
    "for age in ages:\n",
    "    counts.append(np.count_nonzero(data['age']==age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f55ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table with the first 15 most dense samples regarding age\n",
    "type(ages), type(counts)\n",
    "s =pd.DataFrame([ages.T, np.array(counts).T],['ages', 'counts'])\n",
    "s=s.transpose()\n",
    "more_dense = s.sort_values(by=['counts'], ascending=False)\n",
    "more_dense.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a8a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data[data.age==29].index).copy()\n",
    "data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d30e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data.age);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a875dbe4",
   "metadata": {},
   "source": [
    "The filter of Pierre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce2309",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data.copy()\n",
    "data['points_bin'] = pd.qcut(data_clean['age'], q=10)\n",
    "\n",
    "#view updated DataFrame\n",
    "print(data)\n",
    "data['points_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2925d612",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# External image manipulation funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b63a3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde420cb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get image\n",
    "#locattion = \"/home/gonzalez/foto.jpg\" # javier\n",
    "#locattion = \"/home/gonzalez/Paul.jpeg\" # Paul\n",
    "#locattion = \"/home/gonzalez/Konstantine.jpeg\"\n",
    "locattion = \"/home/gonzalez/ping.jpg\" # Paul\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dd0703",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#base_dir = os.path.dirname(locattion)\n",
    "#image = cv2.imread(\"/home/gonzalez/foto.jpg\")\n",
    "#plt.imshow(image, cmap='gray');\n",
    "#print(f'==> image resolution {image.shape}')\n",
    "\n",
    "# from PIL import Image           # this can be used to rotate images\n",
    "# image = Image.open(locattion)\n",
    "\n",
    "# (h, w) = image.shape[:2]\n",
    "# blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "# #blob.shape\n",
    "# blob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3935ab",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "imagePath=locattion\n",
    "image = cv2.imread(imagePath)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "faces = faceCascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor=1.3,\n",
    "    minNeighbors=3,\n",
    "    minSize=(30, 30)\n",
    ")\n",
    "\n",
    "print(\"[INFO] Found {0} Faces.\".format(len(faces)))\n",
    "\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0),1)\n",
    "    roi_color = image[y:y + h, x:x + w]\n",
    "    #print(\"[INFO] Object found. Saving locally.\")\n",
    "    #cv2.imwrite(str(w) + str(h) + '_faces.jpg', roi_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df484e00",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(16,16))\n",
    "\n",
    "ax1.imshow(image) # original image\n",
    "\n",
    "ax2.imshow(roi_color) # recorted original image\n",
    "roi_color.shape\n",
    "\n",
    "img = np.mean(roi_color, axis=2) # black and white image\n",
    "ax3.imshow(img, cmap='gray');\n",
    "\n",
    "img=img[2:,2:]  # remove red line effect\n",
    "\n",
    "res_final = cv2.resize(img, dsize=(48, 48), interpolation=cv2.INTER_LINEAR)\n",
    "ax4.imshow(res_final, cmap='gray');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e2f5f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res_final.shape\n",
    "res_final_ready = np.reshape(res_final, (-1, 48, 48,1))\n",
    "res_final_ready.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c6649",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model.predict(res_final_ready)\n",
    "# index = np.where(model.predict(res_final_ready)==(model.predict(res_final_ready).max()))\n",
    "# index[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48cb79",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#model = models.load_model('Model48_datafiltered/')\n",
    "int(model.predict(res_final_ready)[0][0])\n",
    "\n",
    "#model.predict(res_final_ready)\n",
    "#index = np.where(model.predict(res_final_ready)==(model.predict(res_final_ready).max()))\n",
    "#print(f'slot number {index[1][0]}, correspond to range {index[1][0]*step_size-step_size} to {index[1][0]*step_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7755ee0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# resize image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5413b2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# image conver to black and white\n",
    "#image = cv2.imread(imagePath)\n",
    "#gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475cf968",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Function for transforming data numbers into data range classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a608365",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# categorize age per range:\n",
    "def age_categorize(input_list, age_step=10):\n",
    "    '''\n",
    "    Enter the list of age into input_list and the age steps\n",
    "    with : age_step = 5;  age = 4   =>  1-5\n",
    "                          age = 12  =>  10-15                        \n",
    "    '''\n",
    "    \n",
    "    cat_age = []\n",
    "    for age in input_list:\n",
    "        \n",
    "        a = float(age)/float(age_step)\n",
    "        \n",
    "        if a > 1:\n",
    "            entero = int(a)\n",
    "            coma = a-entero\n",
    "            \n",
    "            if coma > 0:\n",
    "                entero = entero+1\n",
    "            \n",
    "            max = entero * age_step\n",
    "            min = max-(age_step-1)     \n",
    "            #cat_age.append(f'{min} to {max}')   # if the output is in the real intervale\n",
    "            cat_age.append(int(max/age_step)-1)  # if the output is in categorical int number\n",
    "        else:\n",
    "            min = 1\n",
    "            max = age_step\n",
    "            #cat_age.append(f'{min} to {max}')    # if the output is in the real intervale   \n",
    "            cat_age.append(int(max/age_step)-1)   # if the output is in categorical int number\n",
    "\n",
    "            \n",
    "    return cat_age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6033ea",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Here we go with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f3f14",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers, models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e6a003",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X = data['pixels'].tolist()\n",
    "# X = np.reshape(X, (-1, 48, 48,1))\n",
    "\n",
    "\n",
    "X = data['pixels'].tolist()\n",
    "X = np.reshape(X, (-1, 48, 48,3))\n",
    "\n",
    "y=data['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43503b3e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5110f4f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# train_datagen=ImageDataGenerator(rescale=1/255)\n",
    "# train_generator_age=train_datagen.flow(\n",
    "#     X_train ,y_train ,batch_size=32)\n",
    "\n",
    "# test_datagen=ImageDataGenerator(rescale=1/255)\n",
    "# test_generator_age=test_datagen.flow(\n",
    "#     X_test ,y_test ,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a9196d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def initialize_model(numb_int, numb_out):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(48,48,numb_int)))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "   \n",
    "    model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))          \n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='relu'))\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463a3e3",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = initialize_model(X.shape[-1], y.shape[-1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a62db1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de183bdf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='mae', patience=6, restore_best_weights=True)\n",
    "\n",
    "# earlystop=EarlyStopping(patience=6)\n",
    "# learning_rate_reduction=ReduceLROnPlateau(\n",
    "#     monitor='val_acc',\n",
    "#     patience= 3,\n",
    "#     verbose=1,\n",
    "# )\n",
    "# callbacks = [earlystop, learning_rate_reduction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216753a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66b1fda",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# history_age = model.fit(\n",
    "#     train_generator_age, \n",
    "#     epochs= 60,\n",
    "#     validation_data= test_generator_age,\n",
    "#     callbacks= callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839ca07",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#history = model.fit(X_train, y_train, epochs=40, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d014940e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X, y, validation_split=0.3, epochs=40, callbacks=[es], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ca500e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#history.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0f22a2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss']);\n",
    "plt.plot(history.history['mae']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d35cd0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "models.save_model(model, 'Model48_linearColor')\n",
    "#model = models.load_model('Model48')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986d211d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b4158",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n=142\n",
    "plt.imshow(X[n] );\n",
    "data.iloc[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c234dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#out= model.predict(X_test)\n",
    "try_inp = np.expand_dims(X[n], axis=0)\n",
    "model.predict(try_inp)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f0f02f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.shape(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01508e8a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# MODEL USING DATA BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbf9a64",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers, models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57b0e15",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "step_size = 5\n",
    "input_list = data['age']\n",
    "cat = age_categorize(input_list, step_size)\n",
    "#pd.DataFrame(cat, data['age'].values).sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ea20a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add categorical age clasification to original dataframe\n",
    "data['class_age']=cat\n",
    "#data[['age','class_age']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4443bf5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.histplot(data['class_age']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df7c87",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### perform one-hot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse = False)\n",
    "ohe.fit(data[['class_age']])\n",
    "class_age_encoded = ohe.transform(data[['class_age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd15671",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9051a97",
   "metadata": {
    "hidden": true
   },
   "source": [
    " Using Pierre distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39f262b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pierre distribution\n",
    "data_clean = data.copy()\n",
    "data['points_bin'] = pd.qcut(data_clean['age'], q=10)\n",
    "\n",
    "#view updated DataFrame\n",
    "print(data)\n",
    "data['points_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e401d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# perform one-hot encoder to the Pierre distribution\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse = False)\n",
    "ohe.fit(data[['points_bin']])\n",
    "class_age_encoded = ohe.transform(data[['points_bin']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418f2d9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for elements in range(class_age_encoded.shape[1]):      # =====> THIS IS NEED WHATHERVER HOT ENCODER USED  <=====\n",
    "    data[str(elements)]=class_age_encoded[:,elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a0cc5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y=data.drop(columns=['age','ethnicity','gender', 'pixels', 'points_bin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add11fe",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finished Pierre encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d472cd50",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#y=data.drop(columns=['age','ethnicity','gender', 'pixels', 'class_age'])\n",
    "#y=data.drop(columns=['age','ethnicity','gender', 'img_name', 'pixels', 'points_bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4a07d1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = data['pixels'].tolist()\n",
    "X = np.reshape(X, (-1, 48, 48,3))\n",
    "\n",
    "y = class_age_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b1c397",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d2766",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def initialize_model_catgorical(numb_int, numb_out):\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32,(3,3), padding='same',activation='relu',input_shape=(48,48,numb_int)))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    model.add(layers.Conv2D(32,(3,3), padding='same',activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    model.add(layers.Conv2D(64,(3,3), padding='same',activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128,activation='relu'))\n",
    "   \n",
    "    #model.add(layers.Flatten())\n",
    "    #model.add(layers.Dense(128,activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(numb_out, activation='softmax'))   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b5318",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#model = initialize_model_catgorical()\n",
    "model = initialize_model_catgorical(X.shape[-1], y.shape[-1])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d274b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam' ,loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d3c0f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='accuracy', patience=6, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda9868",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "history_cat = model.fit(X_train,y_train, validation_split=0.3, epochs=50, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afad490",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#history_cat = model.fit(X, y, validation_split=0.3, epochs=40, callbacks=[es], batch_size=32)  # the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6883912",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "models.save_model(model, 'Model48_categorical_ColorData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4b4d6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history_cat.history['val_accuracy']);\n",
    "plt.plot(history_cat.history['accuracy']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dda7242",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece4e418",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n=244\n",
    "plt.imshow(X[n], cmap='gray');\n",
    "#np.where(y.iloc[n]==1)[0]\n",
    "print(f\"real age is {data.iloc[n]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed54e9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try_inp = np.expand_dims(X[n], axis=0)\n",
    "model.predict(try_inp).max()\n",
    "index = np.where(model.predict(try_inp)==(model.predict(try_inp).max()))\n",
    "index[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ded3c6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.predict(try_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837fcf02",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try_inp = np.expand_dims(X[n], axis=0)\n",
    "model.predict(try_inp).max()\n",
    "index = np.where(model.predict(try_inp)==(model.predict(try_inp).max()))\n",
    "print(f'slot number {index[1][0]}, correspond to range {index[1][0]*step_size-step_size} to {index[1][0]*step_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64598d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# best results using colab with regression on virgen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bddf859",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import csv\n",
    "\n",
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive')\n",
    "data=pd.read_csv('gdrive/My Drive/age_gender.csv')\n",
    "\n",
    "data['pixels']=data['pixels'].apply(lambda x:  np.array(x.split(), dtype=\"float32\"))\n",
    "\n",
    "from tensorflow.keras import Sequential, layers, models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['pixels'].tolist()\n",
    "X = np.reshape(X, (-1, 48, 48,1))\n",
    "\n",
    "y = data['age']\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(48,48,1)))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "   \n",
    "    model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))          \n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='relu'))\n",
    "   \n",
    "    return model\n",
    "\n",
    "model = initialize_model()\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, callbacks=[es])\n",
    "\n",
    "plt.plot(history.history['loss']);\n",
    "plt.plot(history.history['mae']);\n",
    "\n",
    "n=5\n",
    "out = np.reshape(X_test[n], (48, 48))\n",
    "plt.imshow(out, cmap='gray');\n",
    "y_test.iloc[n]\n",
    "\n",
    "try_inp = np.expand_dims(X_test[n], axis=0)\n",
    "model.predict(try_inp)[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4d127",
   "metadata": {},
   "source": [
    "# Combination of categorical plus linear for BW images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01b6e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import csv\n",
    "from tensorflow.keras import Sequential, layers, models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c3437",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e7299bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_regression():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(48,48,1)))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "   \n",
    "    model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))          \n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='relu'))\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bd67a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_catgorical(numb_int, numb_out):\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32,(3,3), padding='same',activation='relu',input_shape=(48,48,numb_int)))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    model.add(layers.Conv2D(32,(3,3), padding='same',activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    model.add(layers.Conv2D(64,(3,3), padding='same',activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128,activation='relu'))\n",
    "   \n",
    "    #model.add(layers.Flatten())\n",
    "    #model.add(layers.Dense(128,activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(numb_out, activation='softmax'))   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712f0d74",
   "metadata": {},
   "source": [
    "## load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd26d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/gonzalez/Desktop/age_gender/age_gender.csv' # old data\n",
    "data = pd.read_csv(data_path)\n",
    "data['pixels']=data['pixels'].apply(lambda x:  np.array(x.split(), dtype=\"float32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43684fe8",
   "metadata": {},
   "source": [
    "## FILTERING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c8d9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the number of counts per age\n",
    "ages = data['age'].unique()\n",
    "counts = []\n",
    "for age in ages:\n",
    "    counts.append(np.count_nonzero(data['age']==age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ab46acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ages</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>2197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>35</td>\n",
       "      <td>880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>30</td>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>32</td>\n",
       "      <td>664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>40</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>36</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>45</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ages  counts\n",
       "26    26    2197\n",
       "0      1    1123\n",
       "28    28     918\n",
       "36    35     880\n",
       "24    24     859\n",
       "25    25     734\n",
       "31    30     724\n",
       "33    32     664\n",
       "27    27     615\n",
       "29    29     570\n",
       "42    40     526\n",
       "37    36     483\n",
       "19     2     482\n",
       "47    45     440\n",
       "23    23     426"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table with the first 15 most dense samples regarding age\n",
    "type(ages), type(counts)\n",
    "s =pd.DataFrame([ages.T, np.array(counts).T],['ages', 'counts'])\n",
    "s=s.transpose()\n",
    "more_dense = s.sort_values(by=['counts'], ascending=False)\n",
    "more_dense.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec47f333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>gender</th>\n",
       "      <th>img_name</th>\n",
       "      <th>pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110225414790.jpg.chip.jpg</td>\n",
       "      <td>[30.0, 38.0, 50.0, 90.0, 109.0, 113.0, 126.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110225417177.jpg.chip.jpg</td>\n",
       "      <td>[72.0, 81.0, 94.0, 96.0, 77.0, 85.0, 90.0, 71....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110224549512.jpg.chip.jpg</td>\n",
       "      <td>[255.0, 253.0, 252.0, 221.0, 144.0, 174.0, 165...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110225402690.jpg.chip.jpg</td>\n",
       "      <td>[62.0, 53.0, 55.0, 62.0, 73.0, 74.0, 86.0, 94....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110225421531.jpg.chip.jpg</td>\n",
       "      <td>[28.0, 60.0, 55.0, 55.0, 74.0, 74.0, 62.0, 101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19155</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110215653284.jpg.chip.jpg</td>\n",
       "      <td>[73.0, 66.0, 78.0, 108.0, 112.0, 124.0, 128.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19156</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110215848132.jpg.chip.jpg</td>\n",
       "      <td>[74.0, 89.0, 51.0, 63.0, 69.0, 80.0, 94.0, 95....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19157</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110220005370.jpg.chip.jpg</td>\n",
       "      <td>[98.0, 90.0, 90.0, 111.0, 133.0, 149.0, 169.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19158</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110220016235.jpg.chip.jpg</td>\n",
       "      <td>[57.0, 74.0, 93.0, 92.0, 110.0, 124.0, 130.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19159</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110215516060.jpg.chip.jpg</td>\n",
       "      <td>[156.0, 161.0, 139.0, 140.0, 103.0, 89.0, 95.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19160 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  ethnicity  gender                        img_name  \\\n",
       "0       10          0       0  20170110225414790.jpg.chip.jpg   \n",
       "1       10          0       0  20170110225417177.jpg.chip.jpg   \n",
       "2       10          0       0  20170110224549512.jpg.chip.jpg   \n",
       "3       10          0       0  20170110225402690.jpg.chip.jpg   \n",
       "4       10          0       0  20170110225421531.jpg.chip.jpg   \n",
       "...    ...        ...     ...                             ...   \n",
       "19155    9          0       0  20170110215653284.jpg.chip.jpg   \n",
       "19156    9          0       0  20170110215848132.jpg.chip.jpg   \n",
       "19157    9          0       0  20170110220005370.jpg.chip.jpg   \n",
       "19158    9          0       0  20170110220016235.jpg.chip.jpg   \n",
       "19159    9          0       0  20170110215516060.jpg.chip.jpg   \n",
       "\n",
       "                                                  pixels  \n",
       "0      [30.0, 38.0, 50.0, 90.0, 109.0, 113.0, 126.0, ...  \n",
       "1      [72.0, 81.0, 94.0, 96.0, 77.0, 85.0, 90.0, 71....  \n",
       "2      [255.0, 253.0, 252.0, 221.0, 144.0, 174.0, 165...  \n",
       "3      [62.0, 53.0, 55.0, 62.0, 73.0, 74.0, 86.0, 94....  \n",
       "4      [28.0, 60.0, 55.0, 55.0, 74.0, 74.0, 62.0, 101...  \n",
       "...                                                  ...  \n",
       "19155  [73.0, 66.0, 78.0, 108.0, 112.0, 124.0, 128.0,...  \n",
       "19156  [74.0, 89.0, 51.0, 63.0, 69.0, 80.0, 94.0, 95....  \n",
       "19157  [98.0, 90.0, 90.0, 111.0, 133.0, 149.0, 169.0,...  \n",
       "19158  [57.0, 74.0, 93.0, 92.0, 110.0, 124.0, 130.0, ...  \n",
       "19159  [156.0, 161.0, 139.0, 140.0, 103.0, 89.0, 95.0...  \n",
       "\n",
       "[19160 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(data[data.age==26].index).copy()\n",
    "data = data.drop(data[data.age<2].index).copy()\n",
    "data = data.drop(data[data.age>70].index).copy()\n",
    "\n",
    "data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "737e0c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(data.age);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ca3a4",
   "metadata": {},
   "source": [
    "Steping and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3fe75b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 10\n",
    "input_list = data['age']\n",
    "cat = age_categorize(input_list, step_size)  # ====>>  INITIALIZE THIS FUNCTION  <<=========\n",
    "\n",
    "#pd.DataFrame(cat, data['age'].values).sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "affd3a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add categorical age clasification to original dataframe\n",
    "data['points_bin']=cat\n",
    "#data[['age','class_age']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92570744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(data['points_bin']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "716e7eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### perform one-hot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse = False)\n",
    "ohe.fit(data[['points_bin']])\n",
    "class_age_encoded = ohe.transform(data[['points_bin']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "662905ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elements in range(class_age_encoded.shape[1]):      # =====> THIS IS NEED WHATHERVER HOT ENCODER USED  <=====\n",
    "    data[str(elements)]=class_age_encoded[:,elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e4f185f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>gender</th>\n",
       "      <th>img_name</th>\n",
       "      <th>pixels</th>\n",
       "      <th>points_bin</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20196</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170120224611152.jpg.chip.jpg</td>\n",
       "      <td>[166.0, 107.0, 100.0, 80.0, 82.0, 73.0, 62.0, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17413</th>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>20170119205212904.jpg.chip.jpg</td>\n",
       "      <td>[147.0, 146.0, 136.0, 132.0, 145.0, 154.0, 160...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16383</th>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170117154836485.jpg.chip.jpg</td>\n",
       "      <td>[79.0, 99.0, 104.0, 110.0, 124.0, 128.0, 139.0...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9068</th>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20170116172433974.jpg.chip.jpg</td>\n",
       "      <td>[20.0, 20.0, 22.0, 19.0, 23.0, 22.0, 14.0, 75....</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22805</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20170117154607954.jpg.chip.jpg</td>\n",
       "      <td>[81.0, 92.0, 103.0, 103.0, 95.0, 114.0, 132.0,...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8924</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170117175613066.jpg.chip.jpg</td>\n",
       "      <td>[12.0, 11.0, 11.0, 10.0, 10.0, 14.0, 10.0, 13....</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15243</th>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20170104234641563.jpg.chip.jpg</td>\n",
       "      <td>[2.0, 1.0, 2.0, 3.0, 2.0, 3.0, 4.0, 9.0, 22.0,...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20170109203419299.jpg.chip.jpg</td>\n",
       "      <td>[38.0, 44.0, 73.0, 89.0, 119.0, 143.0, 196.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2661</th>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>20170109192214298.jpg.chip.jpg</td>\n",
       "      <td>[149.0, 151.0, 151.0, 153.0, 152.0, 147.0, 114...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13782</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20170108225911130.jpg.chip.jpg</td>\n",
       "      <td>[17.0, 36.0, 52.0, 61.0, 88.0, 108.0, 131.0, 1...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  ethnicity  gender                        img_name  \\\n",
       "20196   58          0       0  20170120224611152.jpg.chip.jpg   \n",
       "17413   47          3       0  20170119205212904.jpg.chip.jpg   \n",
       "16383   42          0       0  20170117154836485.jpg.chip.jpg   \n",
       "9068    28          2       1  20170116172433974.jpg.chip.jpg   \n",
       "22805    8          0       1  20170117154607954.jpg.chip.jpg   \n",
       "8924    27          0       0  20170117175613066.jpg.chip.jpg   \n",
       "15243   39          3       1  20170104234641563.jpg.chip.jpg   \n",
       "2137    16          0       1  20170109203419299.jpg.chip.jpg   \n",
       "2661    19          4       1  20170109192214298.jpg.chip.jpg   \n",
       "13782   35          0       1  20170108225911130.jpg.chip.jpg   \n",
       "\n",
       "                                                  pixels  points_bin    0  \\\n",
       "20196  [166.0, 107.0, 100.0, 80.0, 82.0, 73.0, 62.0, ...           5  0.0   \n",
       "17413  [147.0, 146.0, 136.0, 132.0, 145.0, 154.0, 160...           4  0.0   \n",
       "16383  [79.0, 99.0, 104.0, 110.0, 124.0, 128.0, 139.0...           4  0.0   \n",
       "9068   [20.0, 20.0, 22.0, 19.0, 23.0, 22.0, 14.0, 75....           2  0.0   \n",
       "22805  [81.0, 92.0, 103.0, 103.0, 95.0, 114.0, 132.0,...           0  1.0   \n",
       "8924   [12.0, 11.0, 11.0, 10.0, 10.0, 14.0, 10.0, 13....           2  0.0   \n",
       "15243  [2.0, 1.0, 2.0, 3.0, 2.0, 3.0, 4.0, 9.0, 22.0,...           3  0.0   \n",
       "2137   [38.0, 44.0, 73.0, 89.0, 119.0, 143.0, 196.0, ...           1  0.0   \n",
       "2661   [149.0, 151.0, 151.0, 153.0, 152.0, 147.0, 114...           1  0.0   \n",
       "13782  [17.0, 36.0, 52.0, 61.0, 88.0, 108.0, 131.0, 1...           3  0.0   \n",
       "\n",
       "         1    2    3    4    5    6  \n",
       "20196  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "17413  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "16383  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "9068   0.0  1.0  0.0  0.0  0.0  0.0  \n",
       "22805  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "8924   0.0  1.0  0.0  0.0  0.0  0.0  \n",
       "15243  0.0  0.0  1.0  0.0  0.0  0.0  \n",
       "2137   1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2661   1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "13782  0.0  0.0  1.0  0.0  0.0  0.0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c5865",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## If using piere fiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc70d9e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pierre distribution\n",
    "data_clean = data.copy()\n",
    "data['points_bin'] = pd.qcut(data_clean['age'], q=3)\n",
    "\n",
    "#view updated DataFrame\n",
    "print(data)\n",
    "data['points_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f765eb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# perform one-hot encoder to the Pierre distribution\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse = False)\n",
    "ohe.fit(data[['points_bin']])\n",
    "class_age_encoded = ohe.transform(data[['points_bin']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4439651f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for elements in range(class_age_encoded.shape[1]):\n",
    "    data[str(elements)]=class_age_encoded[:,elements]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d84f9",
   "metadata": {},
   "source": [
    "## categorical fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0ddfe80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['pixels'].tolist()\n",
    "X = np.reshape(X, (-1, 48, 48,1))\n",
    "\n",
    "y=data.drop(columns=['age','ethnicity','gender', 'pixels', 'points_bin', 'img_name'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa903518",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bece510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "294/294 [==============================] - 14s 46ms/step - loss: 2.5584 - accuracy: 0.3204 - val_loss: 1.5902 - val_accuracy: 0.3782\n",
      "Epoch 2/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 1.5127 - accuracy: 0.3951 - val_loss: 1.4612 - val_accuracy: 0.4302\n",
      "Epoch 3/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 1.4158 - accuracy: 0.4297 - val_loss: 1.4026 - val_accuracy: 0.4404\n",
      "Epoch 4/50\n",
      "294/294 [==============================] - 13s 46ms/step - loss: 1.3465 - accuracy: 0.4515 - val_loss: 1.3746 - val_accuracy: 0.4493\n",
      "Epoch 5/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 1.3025 - accuracy: 0.4691 - val_loss: 1.4699 - val_accuracy: 0.4058\n",
      "Epoch 6/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 1.2635 - accuracy: 0.4898 - val_loss: 1.3299 - val_accuracy: 0.4679\n",
      "Epoch 7/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 1.2270 - accuracy: 0.4989 - val_loss: 1.3681 - val_accuracy: 0.4585\n",
      "Epoch 8/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 1.1783 - accuracy: 0.5231 - val_loss: 1.3284 - val_accuracy: 0.4702\n",
      "Epoch 9/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 1.1221 - accuracy: 0.5450 - val_loss: 1.3261 - val_accuracy: 0.4751\n",
      "Epoch 10/50\n",
      "294/294 [==============================] - 14s 47ms/step - loss: 1.0926 - accuracy: 0.5511 - val_loss: 1.3603 - val_accuracy: 0.4573\n",
      "Epoch 11/50\n",
      "294/294 [==============================] - 13s 46ms/step - loss: 1.0401 - accuracy: 0.5742 - val_loss: 1.3859 - val_accuracy: 0.4622\n",
      "Epoch 12/50\n",
      "294/294 [==============================] - 13s 46ms/step - loss: 0.9833 - accuracy: 0.6028 - val_loss: 1.4144 - val_accuracy: 0.4672\n",
      "Epoch 13/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.9275 - accuracy: 0.6284 - val_loss: 1.4811 - val_accuracy: 0.4423\n",
      "Epoch 14/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.8820 - accuracy: 0.6425 - val_loss: 1.6620 - val_accuracy: 0.4436\n",
      "Epoch 15/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.8297 - accuracy: 0.6713 - val_loss: 1.6317 - val_accuracy: 0.4508\n",
      "Epoch 16/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.7668 - accuracy: 0.6962 - val_loss: 1.8017 - val_accuracy: 0.4578\n",
      "Epoch 17/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.7231 - accuracy: 0.7136 - val_loss: 1.8061 - val_accuracy: 0.4486\n",
      "Epoch 18/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.6878 - accuracy: 0.7271 - val_loss: 1.8677 - val_accuracy: 0.4431\n",
      "Epoch 19/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.6199 - accuracy: 0.7614 - val_loss: 2.1035 - val_accuracy: 0.4322\n",
      "Epoch 20/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.5690 - accuracy: 0.7782 - val_loss: 2.1019 - val_accuracy: 0.4426\n",
      "Epoch 21/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.5197 - accuracy: 0.7957 - val_loss: 2.1559 - val_accuracy: 0.4404\n",
      "Epoch 22/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.5172 - accuracy: 0.8041 - val_loss: 2.4333 - val_accuracy: 0.4341\n",
      "Epoch 23/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.4866 - accuracy: 0.8151 - val_loss: 2.3578 - val_accuracy: 0.4456\n",
      "Epoch 24/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.4058 - accuracy: 0.8463 - val_loss: 2.6825 - val_accuracy: 0.4391\n",
      "Epoch 25/50\n",
      "294/294 [==============================] - 14s 46ms/step - loss: 0.3904 - accuracy: 0.8530 - val_loss: 2.8605 - val_accuracy: 0.4381\n",
      "Epoch 26/50\n",
      "294/294 [==============================] - 13s 46ms/step - loss: 0.3441 - accuracy: 0.8706 - val_loss: 2.9374 - val_accuracy: 0.4409\n",
      "Epoch 27/50\n",
      "294/294 [==============================] - 13s 46ms/step - loss: 0.3354 - accuracy: 0.8755 - val_loss: 3.1683 - val_accuracy: 0.4341\n",
      "Epoch 28/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.2951 - accuracy: 0.8921 - val_loss: 3.3131 - val_accuracy: 0.4309\n",
      "Epoch 29/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.2722 - accuracy: 0.9019 - val_loss: 3.4536 - val_accuracy: 0.4235\n",
      "Epoch 30/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.2858 - accuracy: 0.8957 - val_loss: 3.8763 - val_accuracy: 0.4384\n",
      "Epoch 31/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.2783 - accuracy: 0.8993 - val_loss: 3.7969 - val_accuracy: 0.4401\n",
      "Epoch 32/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.2504 - accuracy: 0.9089 - val_loss: 3.7323 - val_accuracy: 0.4297\n",
      "Epoch 33/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.2264 - accuracy: 0.9219 - val_loss: 4.7197 - val_accuracy: 0.4376\n",
      "Epoch 34/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.2118 - accuracy: 0.9295 - val_loss: 4.1513 - val_accuracy: 0.4451\n",
      "Epoch 35/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.2412 - accuracy: 0.9137 - val_loss: 4.3727 - val_accuracy: 0.4140\n",
      "Epoch 36/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.2178 - accuracy: 0.9214 - val_loss: 4.3180 - val_accuracy: 0.4252\n",
      "Epoch 37/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.1981 - accuracy: 0.9313 - val_loss: 4.2130 - val_accuracy: 0.4287\n",
      "Epoch 38/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.2194 - accuracy: 0.9243 - val_loss: 4.3578 - val_accuracy: 0.4212\n",
      "Epoch 39/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.2013 - accuracy: 0.9320 - val_loss: 4.7265 - val_accuracy: 0.4128\n",
      "Epoch 40/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.1973 - accuracy: 0.9304 - val_loss: 4.8353 - val_accuracy: 0.4245\n",
      "Epoch 41/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.1296 - accuracy: 0.9564 - val_loss: 4.6456 - val_accuracy: 0.4312\n",
      "Epoch 42/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.1508 - accuracy: 0.9488 - val_loss: 4.9750 - val_accuracy: 0.4312\n",
      "Epoch 43/50\n",
      "294/294 [==============================] - 13s 46ms/step - loss: 0.1773 - accuracy: 0.9426 - val_loss: 5.1933 - val_accuracy: 0.4399\n",
      "Epoch 44/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.1997 - accuracy: 0.9329 - val_loss: 4.9211 - val_accuracy: 0.4389\n",
      "Epoch 45/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.1478 - accuracy: 0.9489 - val_loss: 5.3170 - val_accuracy: 0.4145\n",
      "Epoch 46/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.1825 - accuracy: 0.9406 - val_loss: 5.1894 - val_accuracy: 0.4220\n",
      "Epoch 47/50\n",
      "294/294 [==============================] - 13s 45ms/step - loss: 0.1571 - accuracy: 0.9510 - val_loss: 5.0886 - val_accuracy: 0.4374\n"
     ]
    }
   ],
   "source": [
    "model_cat = initialize_model_catgorical(X.shape[-1], y.shape[-1])\n",
    "    \n",
    "es = EarlyStopping(monitor='accuracy', patience=6, restore_best_weights=True)\n",
    "\n",
    "model_cat.compile(optimizer='adam' ,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history_cat = model_cat.fit(X_train,y_train, validation_split=0.3, epochs=50, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bddd52ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model_cat_10/assets\n"
     ]
    }
   ],
   "source": [
    "models.save_model(model_cat,'Model_cat_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a8a29bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 2s 10ms/step - loss: 4.6925 - accuracy: 0.4261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.692539215087891, 0.4260612428188324]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cat.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d111701e",
   "metadata": {},
   "source": [
    "# calling once the categorical branch model is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9dc5e9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12149/3079341222.py:4: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  cacho = data.drop(data[data.points_bin<lower].index | data[data.points_bin>=upper].index).copy()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([11, 12, 13, 14, 15, 16, 17, 18, 19, 20])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower = data['points_bin'].unique()[1]\n",
    "upper = data['points_bin'].unique()[2]\n",
    "\n",
    "cacho = data.drop(data[data.points_bin<lower].index | data[data.points_bin>=upper].index).copy()\n",
    "cacho['age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7e9d72e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12149/4259728904.py:17: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  cacho = data.drop(data[data.points_bin<lower].index | data[data.points_bin>=upper].index).copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "*****************************************************************************************\n",
      "STARTING MODEL =======>>>>>> 0 with age range 2 to 10 and (2095, 48, 48, 1) samples\n",
      "*****************************************************************************************\n",
      " \n",
      "Epoch 1/40\n",
      "33/33 [==============================] - 2s 55ms/step - loss: 37.6271 - mae: 5.2998 - val_loss: 34.2591 - val_mae: 5.1909\n",
      "Epoch 2/40\n",
      "33/33 [==============================] - 2s 51ms/step - loss: 34.4727 - mae: 5.2037 - val_loss: 34.2591 - val_mae: 5.1909\n",
      "Epoch 3/40\n",
      "33/33 [==============================] - 2s 52ms/step - loss: 34.4727 - mae: 5.2037 - val_loss: 34.2591 - val_mae: 5.1909\n",
      "Epoch 4/40\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 34.4727 - mae: 5.2037 - val_loss: 34.2591 - val_mae: 5.1909\n",
      "Epoch 5/40\n",
      "33/33 [==============================] - 2s 53ms/step - loss: 34.4727 - mae: 5.2037 - val_loss: 34.2591 - val_mae: 5.1909\n",
      "Epoch 6/40\n",
      "33/33 [==============================] - 2s 54ms/step - loss: 34.4727 - mae: 5.2037 - val_loss: 34.2591 - val_mae: 5.1909\n",
      "Epoch 7/40\n",
      "33/33 [==============================] - 2s 52ms/step - loss: 34.4727 - mae: 5.2037 - val_loss: 34.2591 - val_mae: 5.1909\n",
      "Epoch 8/40\n",
      "33/33 [==============================] - 2s 53ms/step - loss: 34.4727 - mae: 5.2037 - val_loss: 34.2591 - val_mae: 5.1909\n",
      "INFO:tensorflow:Assets written to: Model_linear_2_10/assets\n",
      " \n",
      "*****************************************************************************************\n",
      "STARTING MODEL =======>>>>>> 1 with age range 11 to 20 and (1659, 48, 48, 1) samples\n",
      "*****************************************************************************************\n",
      " \n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12149/4259728904.py:17: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  cacho = data.drop(data[data.points_bin<lower].index | data[data.points_bin>=upper].index).copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 2s 58ms/step - loss: 176.3480 - mae: 11.3718 - val_loss: 21.3121 - val_mae: 3.8810\n",
      "Epoch 2/40\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 26.1087 - mae: 4.1729 - val_loss: 30.8619 - val_mae: 4.8878\n",
      "Epoch 3/40\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 22.0575 - mae: 3.7535 - val_loss: 22.1034 - val_mae: 4.0272\n",
      "Epoch 4/40\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 20.4305 - mae: 3.6611 - val_loss: 25.0190 - val_mae: 4.3355\n",
      "Epoch 5/40\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 20.0371 - mae: 3.5918 - val_loss: 26.2639 - val_mae: 4.4712\n",
      "Epoch 6/40\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 20.5351 - mae: 3.6493 - val_loss: 19.1092 - val_mae: 3.7142\n",
      "Epoch 7/40\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 20.0335 - mae: 3.6384 - val_loss: 23.0262 - val_mae: 4.1466\n",
      "Epoch 8/40\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 18.1348 - mae: 3.4071 - val_loss: 17.2185 - val_mae: 3.5207\n",
      "Epoch 9/40\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 18.8862 - mae: 3.5138 - val_loss: 11.5575 - val_mae: 2.8055\n",
      "Epoch 10/40\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 19.9143 - mae: 3.5953 - val_loss: 30.8764 - val_mae: 4.9173\n",
      "Epoch 11/40\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 19.2233 - mae: 3.5308 - val_loss: 17.4193 - val_mae: 3.5571\n",
      "Epoch 12/40\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 17.1433 - mae: 3.2917 - val_loss: 20.8431 - val_mae: 3.9332\n",
      "Epoch 13/40\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 15.5303 - mae: 3.2034 - val_loss: 20.1702 - val_mae: 3.8748\n",
      "Epoch 14/40\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 16.3478 - mae: 3.2663 - val_loss: 18.7359 - val_mae: 3.6942\n",
      "Epoch 15/40\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 18.8798 - mae: 3.5174 - val_loss: 33.7066 - val_mae: 5.2130\n",
      "Epoch 16/40\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 17.6386 - mae: 3.4017 - val_loss: 13.4551 - val_mae: 3.0755\n",
      "Epoch 17/40\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 16.6967 - mae: 3.2318 - val_loss: 12.6460 - val_mae: 2.9605\n",
      "Epoch 18/40\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 18.5750 - mae: 3.4569 - val_loss: 12.1703 - val_mae: 2.9170\n",
      "Epoch 19/40\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 15.8853 - mae: 3.1846 - val_loss: 11.0418 - val_mae: 2.7563\n",
      "Epoch 20/40\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 14.9570 - mae: 3.0912 - val_loss: 15.7248 - val_mae: 3.3338\n",
      "Epoch 21/40\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 15.8814 - mae: 3.2386 - val_loss: 9.7017 - val_mae: 2.4113\n",
      "Epoch 22/40\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 17.4434 - mae: 3.3564 - val_loss: 12.4896 - val_mae: 2.9494\n",
      "Epoch 23/40\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 15.9620 - mae: 3.1995 - val_loss: 14.5161 - val_mae: 3.1898\n",
      "Epoch 24/40\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 15.2512 - mae: 3.1863 - val_loss: 18.6533 - val_mae: 3.6782\n",
      "Epoch 25/40\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 15.6889 - mae: 3.1299 - val_loss: 12.5220 - val_mae: 2.9224\n",
      "Epoch 26/40\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 15.6328 - mae: 3.1941 - val_loss: 12.3994 - val_mae: 2.9259\n",
      "INFO:tensorflow:Assets written to: Model_linear_11_20/assets\n",
      " \n",
      "*****************************************************************************************\n",
      "STARTING MODEL =======>>>>>> 2 with age range 21 to 30 and (5587, 48, 48, 1) samples\n",
      "*****************************************************************************************\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12149/4259728904.py:17: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  cacho = data.drop(data[data.points_bin<lower].index | data[data.points_bin>=upper].index).copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "86/86 [==============================] - 5s 51ms/step - loss: 93.1119 - mae: 7.3848 - val_loss: 104.0949 - val_mae: 9.4223\n",
      "Epoch 2/40\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 47.2188 - mae: 5.5397 - val_loss: 91.7342 - val_mae: 8.7929\n",
      "Epoch 3/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 43.6209 - mae: 5.3211 - val_loss: 43.7337 - val_mae: 5.5328\n",
      "Epoch 4/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 40.4148 - mae: 5.1250 - val_loss: 54.7385 - val_mae: 6.4305\n",
      "Epoch 5/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 38.4926 - mae: 5.0294 - val_loss: 21.8228 - val_mae: 3.7273\n",
      "Epoch 6/40\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 36.7440 - mae: 4.8509 - val_loss: 62.2673 - val_mae: 7.0225\n",
      "Epoch 7/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 37.0024 - mae: 4.8941 - val_loss: 46.8561 - val_mae: 5.8551\n",
      "Epoch 8/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 35.5790 - mae: 4.7777 - val_loss: 28.2609 - val_mae: 4.3126\n",
      "Epoch 9/40\n",
      "86/86 [==============================] - 5s 57ms/step - loss: 36.2330 - mae: 4.8711 - val_loss: 29.3286 - val_mae: 4.4108\n",
      "Epoch 10/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 34.0525 - mae: 4.7013 - val_loss: 50.0764 - val_mae: 6.1774\n",
      "Epoch 11/40\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 33.5449 - mae: 4.6408 - val_loss: 21.8607 - val_mae: 3.7513\n",
      "Epoch 12/40\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 34.5188 - mae: 4.7261 - val_loss: 23.5913 - val_mae: 3.9229\n",
      "Epoch 13/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 33.6248 - mae: 4.6431 - val_loss: 34.6130 - val_mae: 4.9208\n",
      "Epoch 14/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 33.8452 - mae: 4.6882 - val_loss: 15.6089 - val_mae: 3.1821\n",
      "Epoch 15/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 32.1248 - mae: 4.5921 - val_loss: 20.1434 - val_mae: 3.6063\n",
      "Epoch 16/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 33.3492 - mae: 4.6471 - val_loss: 19.3005 - val_mae: 3.5434\n",
      "Epoch 17/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 33.2796 - mae: 4.6460 - val_loss: 31.5489 - val_mae: 4.6891\n",
      "Epoch 18/40\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 30.9472 - mae: 4.4726 - val_loss: 27.6253 - val_mae: 4.3280\n",
      "Epoch 19/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 29.8582 - mae: 4.3875 - val_loss: 37.7038 - val_mae: 5.2517\n",
      "Epoch 20/40\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 32.8548 - mae: 4.6241 - val_loss: 18.1709 - val_mae: 3.4373\n",
      "Epoch 21/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 29.7005 - mae: 4.3688 - val_loss: 20.0848 - val_mae: 3.6299\n",
      "Epoch 22/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 30.3905 - mae: 4.4220 - val_loss: 20.7672 - val_mae: 3.7120\n",
      "Epoch 23/40\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 28.2309 - mae: 4.2810 - val_loss: 17.2186 - val_mae: 3.3641\n",
      "Epoch 24/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 27.7067 - mae: 4.2663 - val_loss: 14.1325 - val_mae: 3.0452\n",
      "Epoch 25/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 27.2822 - mae: 4.2005 - val_loss: 22.6274 - val_mae: 3.9256\n",
      "Epoch 26/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 26.5989 - mae: 4.1262 - val_loss: 12.4479 - val_mae: 2.8821\n",
      "Epoch 27/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 26.5432 - mae: 4.1301 - val_loss: 24.8605 - val_mae: 4.1611\n",
      "Epoch 28/40\n",
      "86/86 [==============================] - 4s 51ms/step - loss: 26.6223 - mae: 4.1106 - val_loss: 12.5008 - val_mae: 2.9122\n",
      "Epoch 29/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 26.5588 - mae: 4.1617 - val_loss: 12.9882 - val_mae: 2.9588\n",
      "Epoch 30/40\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 25.0250 - mae: 3.9902 - val_loss: 12.0540 - val_mae: 2.8403\n",
      "Epoch 31/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 26.4030 - mae: 4.1149 - val_loss: 10.8567 - val_mae: 2.7208\n",
      "Epoch 32/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 24.6821 - mae: 3.9669 - val_loss: 13.8063 - val_mae: 3.0542\n",
      "Epoch 33/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 24.8340 - mae: 3.9604 - val_loss: 10.6800 - val_mae: 2.7169\n",
      "Epoch 34/40\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 22.6551 - mae: 3.8417 - val_loss: 9.9512 - val_mae: 2.6289\n",
      "Epoch 35/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 23.4758 - mae: 3.9019 - val_loss: 11.8969 - val_mae: 2.8385\n",
      "Epoch 36/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 22.5539 - mae: 3.8159 - val_loss: 10.9458 - val_mae: 2.7452\n",
      "Epoch 37/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 22.7916 - mae: 3.8391 - val_loss: 9.6576 - val_mae: 2.6096\n",
      "Epoch 38/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 23.9401 - mae: 3.9278 - val_loss: 11.4245 - val_mae: 2.8172\n",
      "Epoch 39/40\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 23.3189 - mae: 3.8909 - val_loss: 14.2120 - val_mae: 3.1161\n",
      "Epoch 40/40\n",
      "86/86 [==============================] - 5s 55ms/step - loss: 22.3624 - mae: 3.7995 - val_loss: 9.7521 - val_mae: 2.6165\n",
      "INFO:tensorflow:Assets written to: Model_linear_21_30/assets\n",
      " \n",
      "*****************************************************************************************\n",
      "STARTING MODEL =======>>>>>> 3 with age range 31 to 40 and (4338, 48, 48, 1) samples\n",
      "*****************************************************************************************\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12149/4259728904.py:17: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  cacho = data.drop(data[data.points_bin<lower].index | data[data.points_bin>=upper].index).copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "67/67 [==============================] - 4s 58ms/step - loss: 151.8703 - mae: 9.5505 - val_loss: 70.1125 - val_mae: 6.9793\n",
      "Epoch 2/40\n",
      "67/67 [==============================] - 4s 57ms/step - loss: 81.8252 - mae: 7.2832 - val_loss: 51.6337 - val_mae: 5.7777\n",
      "Epoch 3/40\n",
      "67/67 [==============================] - 4s 53ms/step - loss: 79.8201 - mae: 7.1984 - val_loss: 42.5600 - val_mae: 5.1569\n",
      "Epoch 4/40\n",
      "67/67 [==============================] - 4s 54ms/step - loss: 72.6316 - mae: 6.9203 - val_loss: 47.0208 - val_mae: 5.4605\n",
      "Epoch 5/40\n",
      "67/67 [==============================] - 4s 55ms/step - loss: 77.3397 - mae: 7.0758 - val_loss: 71.1899 - val_mae: 7.1901\n",
      "Epoch 6/40\n",
      "67/67 [==============================] - 4s 55ms/step - loss: 67.1034 - mae: 6.5672 - val_loss: 66.4528 - val_mae: 6.8938\n",
      "Epoch 7/40\n",
      "67/67 [==============================] - 4s 57ms/step - loss: 66.9983 - mae: 6.6425 - val_loss: 68.3306 - val_mae: 7.0662\n",
      "Epoch 8/40\n",
      "67/67 [==============================] - 4s 58ms/step - loss: 62.0174 - mae: 6.3797 - val_loss: 45.6387 - val_mae: 5.4683\n",
      "Epoch 9/40\n",
      "67/67 [==============================] - 4s 53ms/step - loss: 65.8518 - mae: 6.5195 - val_loss: 40.2020 - val_mae: 5.0610\n",
      "Epoch 10/40\n",
      "67/67 [==============================] - 3s 52ms/step - loss: 61.3973 - mae: 6.3613 - val_loss: 41.7562 - val_mae: 5.1992\n",
      "Epoch 11/40\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 63.5225 - mae: 6.3896 - val_loss: 37.5110 - val_mae: 4.9006\n",
      "Epoch 12/40\n",
      "67/67 [==============================] - 3s 51ms/step - loss: 64.3625 - mae: 6.4890 - val_loss: 25.5203 - val_mae: 3.9280\n",
      "Epoch 13/40\n",
      "67/67 [==============================] - 3s 51ms/step - loss: 60.4354 - mae: 6.1793 - val_loss: 27.1926 - val_mae: 4.0733\n",
      "Epoch 14/40\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 56.9478 - mae: 6.0516 - val_loss: 32.3639 - val_mae: 4.5342\n",
      "Epoch 15/40\n",
      "67/67 [==============================] - 3s 49ms/step - loss: 60.4954 - mae: 6.2158 - val_loss: 24.7848 - val_mae: 3.9769\n",
      "Epoch 16/40\n",
      "67/67 [==============================] - 3s 51ms/step - loss: 62.0316 - mae: 6.3283 - val_loss: 22.8527 - val_mae: 3.8003\n",
      "Epoch 17/40\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 55.8390 - mae: 5.9996 - val_loss: 48.4441 - val_mae: 5.8692\n",
      "Epoch 18/40\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 54.8407 - mae: 5.9818 - val_loss: 20.8391 - val_mae: 3.5914\n",
      "Epoch 19/40\n",
      "67/67 [==============================] - 3s 51ms/step - loss: 52.8605 - mae: 5.8376 - val_loss: 26.3924 - val_mae: 4.0847\n",
      "Epoch 20/40\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 51.3712 - mae: 5.7423 - val_loss: 41.5080 - val_mae: 5.3721\n",
      "Epoch 21/40\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 50.5327 - mae: 5.6608 - val_loss: 29.3564 - val_mae: 4.3686\n",
      "Epoch 22/40\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 50.1343 - mae: 5.6593 - val_loss: 21.8091 - val_mae: 3.7135\n",
      "Epoch 23/40\n",
      "67/67 [==============================] - 3s 52ms/step - loss: 45.9749 - mae: 5.4535 - val_loss: 19.5866 - val_mae: 3.5094\n",
      "Epoch 24/40\n",
      "67/67 [==============================] - 3s 52ms/step - loss: 45.6191 - mae: 5.4843 - val_loss: 18.2823 - val_mae: 3.4035\n",
      "Epoch 25/40\n",
      "67/67 [==============================] - 3s 51ms/step - loss: 43.0113 - mae: 5.2828 - val_loss: 20.0587 - val_mae: 3.6145\n",
      "Epoch 26/40\n",
      "67/67 [==============================] - 3s 51ms/step - loss: 42.1558 - mae: 5.2448 - val_loss: 13.6576 - val_mae: 3.0072\n",
      "Epoch 27/40\n",
      "67/67 [==============================] - 3s 49ms/step - loss: 38.1381 - mae: 4.9464 - val_loss: 14.1109 - val_mae: 2.9866\n",
      "Epoch 28/40\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 38.7414 - mae: 5.0088 - val_loss: 12.5383 - val_mae: 2.8319\n",
      "Epoch 29/40\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 37.9353 - mae: 4.9595 - val_loss: 10.7481 - val_mae: 2.6389\n",
      "Epoch 30/40\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 39.0219 - mae: 5.0034 - val_loss: 10.9397 - val_mae: 2.6729\n",
      "Epoch 31/40\n",
      "67/67 [==============================] - 3s 51ms/step - loss: 39.4228 - mae: 5.0183 - val_loss: 10.8183 - val_mae: 2.6523\n",
      "Epoch 32/40\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 36.1426 - mae: 4.7987 - val_loss: 12.5641 - val_mae: 2.8651\n",
      "Epoch 33/40\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 37.1980 - mae: 4.8766 - val_loss: 13.6891 - val_mae: 2.9961\n",
      "Epoch 34/40\n",
      "67/67 [==============================] - 3s 51ms/step - loss: 37.5669 - mae: 4.8992 - val_loss: 13.9150 - val_mae: 3.0130\n",
      "Epoch 35/40\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 36.6609 - mae: 4.8642 - val_loss: 13.5226 - val_mae: 2.9822\n",
      "Epoch 36/40\n",
      "67/67 [==============================] - 3s 49ms/step - loss: 36.8962 - mae: 4.8329 - val_loss: 12.6674 - val_mae: 2.8950\n",
      "Epoch 37/40\n",
      "67/67 [==============================] - 3s 49ms/step - loss: 35.2839 - mae: 4.7644 - val_loss: 10.3680 - val_mae: 2.6221\n",
      "Epoch 38/40\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 36.7280 - mae: 4.8598 - val_loss: 10.3267 - val_mae: 2.6127\n",
      "Epoch 39/40\n",
      "67/67 [==============================] - 3s 50ms/step - loss: 38.1520 - mae: 4.9660 - val_loss: 9.2490 - val_mae: 2.4854\n",
      "Epoch 40/40\n",
      "67/67 [==============================] - 3s 51ms/step - loss: 35.8586 - mae: 4.8084 - val_loss: 13.7168 - val_mae: 3.0003\n",
      "INFO:tensorflow:Assets written to: Model_linear_31_40/assets\n",
      " \n",
      "*****************************************************************************************\n",
      "STARTING MODEL =======>>>>>> 4 with age range 41 to 50 and (2100, 48, 48, 1) samples\n",
      "*****************************************************************************************\n",
      " \n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12149/4259728904.py:17: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  cacho = data.drop(data[data.points_bin<lower].index | data[data.points_bin>=upper].index).copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 3s 65ms/step - loss: 343.9244 - mae: 14.3348 - val_loss: 78.9123 - val_mae: 7.3090\n",
      "Epoch 2/40\n",
      "33/33 [==============================] - 2s 53ms/step - loss: 151.7918 - mae: 10.0048 - val_loss: 159.1344 - val_mae: 11.3277\n",
      "Epoch 3/40\n",
      "33/33 [==============================] - 2s 51ms/step - loss: 147.6188 - mae: 9.8278 - val_loss: 140.7015 - val_mae: 10.4954\n",
      "Epoch 4/40\n",
      "33/33 [==============================] - 2s 52ms/step - loss: 144.5565 - mae: 9.8450 - val_loss: 94.0317 - val_mae: 8.1536\n",
      "Epoch 5/40\n",
      "33/33 [==============================] - 2s 52ms/step - loss: 130.8853 - mae: 9.1590 - val_loss: 119.5683 - val_mae: 9.5056\n",
      "Epoch 6/40\n",
      "33/33 [==============================] - 2s 52ms/step - loss: 129.2013 - mae: 9.2474 - val_loss: 126.9339 - val_mae: 9.9155\n",
      "Epoch 7/40\n",
      "33/33 [==============================] - 2s 52ms/step - loss: 135.0017 - mae: 9.4096 - val_loss: 88.5473 - val_mae: 7.9456\n",
      "Epoch 8/40\n",
      "33/33 [==============================] - 2s 52ms/step - loss: 132.2071 - mae: 9.2290 - val_loss: 58.4144 - val_mae: 6.1810\n",
      "Epoch 9/40\n",
      "33/33 [==============================] - 2s 50ms/step - loss: 121.2975 - mae: 8.9870 - val_loss: 65.2972 - val_mae: 6.5926\n",
      "Epoch 10/40\n",
      "33/33 [==============================] - 2s 52ms/step - loss: 115.9652 - mae: 8.6840 - val_loss: 44.7990 - val_mae: 5.3712\n",
      "Epoch 11/40\n",
      "33/33 [==============================] - 2s 51ms/step - loss: 123.3951 - mae: 8.9431 - val_loss: 40.4128 - val_mae: 5.0544\n",
      "Epoch 12/40\n",
      "33/33 [==============================] - 2s 51ms/step - loss: 124.6836 - mae: 9.0847 - val_loss: 44.4770 - val_mae: 5.3611\n",
      "Epoch 13/40\n",
      "33/33 [==============================] - 2s 50ms/step - loss: 111.5565 - mae: 8.4997 - val_loss: 53.2534 - val_mae: 5.8991\n",
      "Epoch 14/40\n",
      "33/33 [==============================] - 2s 51ms/step - loss: 113.8460 - mae: 8.6077 - val_loss: 124.5571 - val_mae: 9.9107\n",
      "Epoch 15/40\n",
      "33/33 [==============================] - 2s 50ms/step - loss: 110.1681 - mae: 8.3867 - val_loss: 39.0911 - val_mae: 4.9430\n",
      "Epoch 16/40\n",
      "33/33 [==============================] - 2s 52ms/step - loss: 115.7731 - mae: 8.6966 - val_loss: 70.3285 - val_mae: 6.9515\n",
      "Epoch 17/40\n",
      "33/33 [==============================] - 2s 51ms/step - loss: 116.8938 - mae: 8.5994 - val_loss: 43.6919 - val_mae: 5.2846\n",
      "Epoch 18/40\n",
      "33/33 [==============================] - 2s 52ms/step - loss: 114.5549 - mae: 8.5384 - val_loss: 75.7978 - val_mae: 7.2497\n",
      "Epoch 19/40\n",
      "33/33 [==============================] - 2s 50ms/step - loss: 94.9588 - mae: 7.7803 - val_loss: 59.5700 - val_mae: 6.3007\n",
      "Epoch 20/40\n",
      "33/33 [==============================] - 2s 53ms/step - loss: 100.2529 - mae: 8.1288 - val_loss: 59.4420 - val_mae: 6.2296\n",
      "Epoch 21/40\n",
      "33/33 [==============================] - 2s 51ms/step - loss: 101.3158 - mae: 8.1154 - val_loss: 92.1149 - val_mae: 8.1635\n",
      "Epoch 22/40\n",
      "33/33 [==============================] - 2s 54ms/step - loss: 119.9260 - mae: 8.8206 - val_loss: 42.6086 - val_mae: 5.1754\n",
      "Epoch 23/40\n",
      "33/33 [==============================] - 2s 52ms/step - loss: 97.9339 - mae: 7.8907 - val_loss: 42.2923 - val_mae: 5.1182\n",
      "Epoch 24/40\n",
      "33/33 [==============================] - 2s 51ms/step - loss: 109.5884 - mae: 8.3892 - val_loss: 67.9027 - val_mae: 6.8046\n",
      "Epoch 25/40\n",
      "33/33 [==============================] - 2s 51ms/step - loss: 95.8856 - mae: 7.9141 - val_loss: 44.2543 - val_mae: 5.3042\n",
      "INFO:tensorflow:Assets written to: Model_linear_41_50/assets\n",
      " \n",
      "*****************************************************************************************\n",
      "STARTING MODEL =======>>>>>> 5 with age range 51 to 60 and (2211, 48, 48, 1) samples\n",
      "*****************************************************************************************\n",
      " \n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12149/4259728904.py:17: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  cacho = data.drop(data[data.points_bin<lower].index | data[data.points_bin>=upper].index).copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 2s 56ms/step - loss: 461.6826 - mae: 16.8186 - val_loss: 309.9901 - val_mae: 16.4180\n",
      "Epoch 2/40\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 193.1326 - mae: 11.2592 - val_loss: 187.6344 - val_mae: 12.0700\n",
      "Epoch 3/40\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 183.1595 - mae: 10.6981 - val_loss: 249.7050 - val_mae: 14.4414\n",
      "Epoch 4/40\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 176.1805 - mae: 10.8327 - val_loss: 131.7844 - val_mae: 9.6717\n",
      "Epoch 5/40\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 162.8271 - mae: 10.2393 - val_loss: 154.4710 - val_mae: 10.7400\n",
      "Epoch 6/40\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 176.8447 - mae: 10.6689 - val_loss: 196.9455 - val_mae: 12.5229\n",
      "Epoch 7/40\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 161.0581 - mae: 10.1389 - val_loss: 139.2899 - val_mae: 10.0892\n",
      "Epoch 8/40\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 166.7420 - mae: 10.3567 - val_loss: 193.4592 - val_mae: 12.4408\n",
      "Epoch 9/40\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 155.6302 - mae: 9.9548 - val_loss: 168.5431 - val_mae: 11.4250\n",
      "Epoch 10/40\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 153.3865 - mae: 9.9593 - val_loss: 127.0571 - val_mae: 9.5744\n",
      "Epoch 11/40\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 158.1464 - mae: 10.2445 - val_loss: 112.5550 - val_mae: 8.8689\n",
      "Epoch 12/40\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 158.5104 - mae: 9.9707 - val_loss: 69.7996 - val_mae: 6.5834\n",
      "Epoch 13/40\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 159.9875 - mae: 10.1537 - val_loss: 121.3775 - val_mae: 9.3220\n",
      "Epoch 14/40\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 145.5196 - mae: 9.7267 - val_loss: 213.3629 - val_mae: 13.3057\n",
      "Epoch 15/40\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 154.7626 - mae: 10.0186 - val_loss: 68.7447 - val_mae: 6.5377\n",
      "Epoch 16/40\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 132.7306 - mae: 9.1903 - val_loss: 92.7442 - val_mae: 7.9107\n",
      "Epoch 17/40\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 143.2301 - mae: 9.5745 - val_loss: 68.2722 - val_mae: 6.5488\n",
      "Epoch 18/40\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 148.2221 - mae: 9.6612 - val_loss: 72.3009 - val_mae: 6.7900\n",
      "Epoch 19/40\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 141.4302 - mae: 9.4208 - val_loss: 116.6374 - val_mae: 9.1285\n",
      "Epoch 20/40\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 138.9885 - mae: 9.4704 - val_loss: 103.2420 - val_mae: 8.5022\n",
      "Epoch 21/40\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 141.9444 - mae: 9.6065 - val_loss: 88.1411 - val_mae: 7.7191\n",
      "Epoch 22/40\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 131.5361 - mae: 9.2894 - val_loss: 78.6004 - val_mae: 7.1639\n",
      "INFO:tensorflow:Assets written to: Model_linear_51_60/assets\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of bounds for axis 0 with size 7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12149/4259728904.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mlower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'points_bin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# ====>>  USING JAVIER FORMAT  <<=========\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mupper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'points_bin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mcacho\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoints_bin\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoints_bin\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 7 is out of bounds for axis 0 with size 7"
     ]
    }
   ],
   "source": [
    "# looping for all linear model\n",
    "Histories = []\n",
    "\n",
    "y = data['age']\n",
    "\n",
    "for i in range(len(data['points_bin'].unique())):\n",
    "\n",
    "\n",
    "    # slice the dataframe\n",
    "    \n",
    "    #lower = data['points_bin'].unique()[i].left  # ====>>  USING PIERRE FORMAT  <<=========\n",
    "    #upper = data['points_bin'].unique()[i].right\n",
    "    #cacho = data.drop(data[data.age<lower].index | data[data.age>=upper].index).copy()\n",
    "    \n",
    "    lower = data['points_bin'].unique()[i]   # ====>>  USING JAVIER FORMAT  <<=========\n",
    "    upper = data['points_bin'].unique()[i+1]\n",
    "    cacho = data.drop(data[data.points_bin<lower].index | data[data.points_bin>=upper].index).copy()\n",
    "\n",
    "    \n",
    "    # prepare the data\n",
    "    X = cacho['pixels'].tolist()\n",
    "    X = np.reshape(X, (-1, 48, 48,1))\n",
    "\n",
    "    y=cacho['age']\n",
    "\n",
    "    # split data set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.3, random_state=1)\n",
    "    \n",
    "    \n",
    "    print(' ')\n",
    "    print('*****************************************************************************************')\n",
    "    print(f'STARTING MODEL =======>>>>>> {i} with age range {y.min()} to {y.max()} and {X.shape} samples')\n",
    "    print('*****************************************************************************************')\n",
    "    print(' ')\n",
    "\n",
    "\n",
    "    # initialize the model\n",
    "    model = initialize_model_regression()\n",
    "        \n",
    "    # compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    # early stopping\n",
    "    es = EarlyStopping(monitor='mae', patience=6, restore_best_weights=True)\n",
    "\n",
    "\n",
    "    # fit\n",
    "    history = model.fit(X_train, y_train, validation_split=0.3, epochs=40, callbacks=[es])\n",
    "    \n",
    "    # save model\n",
    "    Histories.append(history)\n",
    "    \n",
    "    models.save_model(model, f'Model_linear_{y.min()}_{y.max()}')\n",
    "    \n",
    "    # delete variables to save RAM\n",
    "    del model, X, y, X_train, X_test, y_train, y_test, es, history, cacho\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d97d3a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870e4f17",
   "metadata": {},
   "source": [
    "## Evaluation test categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8b247360",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['pixels'].tolist()\n",
    "X = np.reshape(X, (-1, 48, 48,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e33abb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real age is age                                                           4\n",
      "ethnicity                                                     2\n",
      "gender                                                        0\n",
      "img_name                         20161219211434893.jpg.chip.jpg\n",
      "pixels        [116.0, 127.0, 132.0, 136.0, 139.0, 144.0, 150...\n",
      "points_bin                                                    0\n",
      "0                                                           1.0\n",
      "1                                                           0.0\n",
      "2                                                           0.0\n",
      "3                                                           0.0\n",
      "4                                                           0.0\n",
      "5                                                           0.0\n",
      "6                                                           0.0\n",
      "Name: 15359, dtype: object\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=12007\n",
    "plt.imshow(X[n], cmap='gray');\n",
    "#np.where(y.iloc[n]==1)[0]\n",
    "print(f\"real age is {data.iloc[n]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2086f798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slot number 0, correspond to range 0 to 10\n"
     ]
    }
   ],
   "source": [
    "try_inp = np.expand_dims(X[n], axis=0)\n",
    "model_cat.predict(try_inp).max()\n",
    "index = np.where(model_cat.predict(try_inp)==(model_cat.predict(try_inp).max()))\n",
    "print(f'slot number {index[1][0]}, correspond to range {(index[1][0]+1)*step_size-step_size} to {(index[1][0]+1)*step_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22747822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_cat.predict(try_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8bf66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_inp = np.expand_dims(X[n], axis=0)\n",
    "# model_cat.predict(try_inp).max()\n",
    "# index = np.where(model_cat.predict(try_inp)==(model_cat.predict(try_inp).max()))\n",
    "# index[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8370a375",
   "metadata": {},
   "source": [
    "## Evaluation test regresional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c94327e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_gender.csv:Zone.Identifier\tModel_linear_2_10   Model_linear_51_60\r\n",
      "images\t\t\t\tModel_linear_21_30  Paul_Gender.ipynb\r\n",
      "Javier_age.ipynb\t\tModel_linear_31_40  Pierre-Ethnicity.ipynb\r\n",
      "Model_linear_11_20\t\tModel_linear_41_50\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "66c4ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corresponding regresional model\n",
    "predict_model = models.load_model('Model_linear_1_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "464f0f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24.807053]], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict regresional\n",
    "predict_model.predict(try_inp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
