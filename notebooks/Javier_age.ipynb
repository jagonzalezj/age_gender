{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b19dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = '/home/gonzalez/Desktop/age_gender/age_gender.csv' # old data\n",
    "# url = 'https://www.kaggle.com/code/shahraizanwar/age-gender-ethnicity-prediction/data?select=age_gender.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c219bbff",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# processing color data from Pierre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61de7b9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    " # new data from Pierre\n",
    "data_path = '/home/gonzalez/code/jagonzalezj/age_gender/raw_data/new_data.csv' \n",
    "data = pd.read_csv(data_path, encoding='utf-8')\n",
    "\n",
    "lola =[]   \n",
    "for i in range(len(data['image'])):\n",
    "    a = data['image'][i].replace('[',',').replace(']',',').replace(',','').split()\n",
    "    lola.append([int(j) for j in a])\n",
    "    \n",
    "data['image']=lola\n",
    "data.columns=['age', 'gender', 'ethnicity', 'pixels']\n",
    "\n",
    "data['ethnicity'].unique()[5:]\n",
    "for items in data['ethnicity'].unique()[5:]:\n",
    "    data = data.drop(data[data.ethnicity==items].index).copy()\n",
    "\n",
    "data = data.dropna()\n",
    "data.reindex()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec1061",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Loading data from google cloud to google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612cad54",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive')\n",
    "df=pd.read_csv('gdrive/My Drive/age_gender.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de094b9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Transforming the pixels data type into a list of float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67406a11",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# images =[]\n",
    "# for fotos in range(len(data['pixels'])):\n",
    "#     X = data['pixels'][fotos].split(\" \")\n",
    "#     X = list(map(int, X))\n",
    "#     images.append(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c382e670",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# x = np.reshape(images[5000], (48,48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2034415d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data['pixels']=data['pixels'].apply(lambda x:  np.array(x.split(), dtype=\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1367365",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "blob = data['pixels'][0].reshape(48,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c74751",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(blob, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a953707",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#sns.displot(data['ethnicity']),\n",
    "#sns.displot(data['gender']), \n",
    "#sns.displot(data['age']);\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 5))\n",
    "sns.histplot(ax=axes[0], x=data['age']);\n",
    "sns.histplot(ax=axes[1], x=data['ethnicity']);\n",
    "sns.histplot(ax=axes[2], x=data['gender']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3251d861",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Working with the age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2f14c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# list the number of counts per age\n",
    "ages = data['age'].unique()\n",
    "counts = []\n",
    "for age in ages:\n",
    "    counts.append(np.count_nonzero(data['age']==age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f55ada",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# table with the first 15 most dense samples regarding age\n",
    "type(ages), type(counts)\n",
    "s =pd.DataFrame([ages.T, np.array(counts).T],['ages', 'counts'])\n",
    "s=s.transpose()\n",
    "more_dense = s.sort_values(by=['counts'], ascending=False)\n",
    "more_dense.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a8a04",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = data.drop(data[data.age==29].index).copy()\n",
    "data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d30e9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.histplot(data.age);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a875dbe4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The filter of Pierre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce2309",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_clean = data.copy()\n",
    "data['points_bin'] = pd.qcut(data_clean['age'], q=10)\n",
    "\n",
    "#view updated DataFrame\n",
    "print(data)\n",
    "data['points_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039097f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2925d612",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# External image manipulation funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b63a3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde420cb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get image\n",
    "#locattion = \"/home/gonzalez/foto.jpg\" # javier\n",
    "#locattion = \"/home/gonzalez/Paul.jpeg\" # Paul\n",
    "#locattion = \"/home/gonzalez/Konstantine.jpeg\"\n",
    "locattion = \"/home/gonzalez/ping.jpg\" # Paul\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dd0703",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#base_dir = os.path.dirname(locattion)\n",
    "#image = cv2.imread(\"/home/gonzalez/foto.jpg\")\n",
    "#plt.imshow(image, cmap='gray');\n",
    "#print(f'==> image resolution {image.shape}')\n",
    "\n",
    "# from PIL import Image           # this can be used to rotate images\n",
    "# image = Image.open(locattion)\n",
    "\n",
    "# (h, w) = image.shape[:2]\n",
    "# blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "# #blob.shape\n",
    "# blob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3935ab",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "imagePath=locattion\n",
    "image = cv2.imread(imagePath)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "faces = faceCascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor=1.3,\n",
    "    minNeighbors=3,\n",
    "    minSize=(30, 30)\n",
    ")\n",
    "\n",
    "print(\"[INFO] Found {0} Faces.\".format(len(faces)))\n",
    "\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0),1)\n",
    "    roi_color = image[y:y + h, x:x + w]\n",
    "    #print(\"[INFO] Object found. Saving locally.\")\n",
    "    #cv2.imwrite(str(w) + str(h) + '_faces.jpg', roi_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df484e00",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(16,16))\n",
    "\n",
    "ax1.imshow(image) # original image\n",
    "\n",
    "ax2.imshow(roi_color) # recorted original image\n",
    "roi_color.shape\n",
    "\n",
    "img = np.mean(roi_color, axis=2) # black and white image\n",
    "ax3.imshow(img, cmap='gray');\n",
    "\n",
    "img=img[2:,2:]  # remove red line effect\n",
    "\n",
    "res_final = cv2.resize(img, dsize=(48, 48), interpolation=cv2.INTER_LINEAR)\n",
    "ax4.imshow(res_final, cmap='gray');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e2f5f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res_final.shape\n",
    "res_final_ready = np.reshape(res_final, (-1, 48, 48,1))\n",
    "res_final_ready.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c6649",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model.predict(res_final_ready)\n",
    "# index = np.where(model.predict(res_final_ready)==(model.predict(res_final_ready).max()))\n",
    "# index[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48cb79",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#model = models.load_model('Model48_datafiltered/')\n",
    "int(model.predict(res_final_ready)[0][0])\n",
    "\n",
    "#model.predict(res_final_ready)\n",
    "#index = np.where(model.predict(res_final_ready)==(model.predict(res_final_ready).max()))\n",
    "#print(f'slot number {index[1][0]}, correspond to range {index[1][0]*step_size-step_size} to {index[1][0]*step_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7755ee0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# resize image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5413b2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# image conver to black and white\n",
    "#image = cv2.imread(imagePath)\n",
    "#gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475cf968",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Function for transforming data numbers into data range classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a608365",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# categorize age per range:\n",
    "def age_categorize(input_list, age_step=10):\n",
    "    '''\n",
    "    Enter the list of age into input_list and the age steps\n",
    "    with : age_step = 5;  age = 4   =>  1-5\n",
    "                          age = 12  =>  10-15                        \n",
    "    '''\n",
    "    \n",
    "    cat_age = []\n",
    "    for age in input_list:\n",
    "        \n",
    "        a = float(age)/float(age_step)\n",
    "        \n",
    "        if a > 1:\n",
    "            entero = int(a)\n",
    "            coma = a-entero\n",
    "            \n",
    "            if coma > 0:\n",
    "                entero = entero+1\n",
    "            \n",
    "            max = entero * age_step\n",
    "            min = max-(age_step-1)     \n",
    "            #cat_age.append(f'{min} to {max}')   # if the output is in the real intervale\n",
    "            cat_age.append(int(max/age_step)-1)  # if the output is in categorical int number\n",
    "        else:\n",
    "            min = 1\n",
    "            max = age_step\n",
    "            #cat_age.append(f'{min} to {max}')    # if the output is in the real intervale   \n",
    "            cat_age.append(int(max/age_step)-1)   # if the output is in categorical int number\n",
    "\n",
    "            \n",
    "    return cat_age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6033ea",
   "metadata": {},
   "source": [
    "# Here we go with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f3f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers, models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e6a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data['pixels'].tolist()\n",
    "# X = np.reshape(X, (-1, 48, 48,1))\n",
    "\n",
    "\n",
    "X = data['pixels'].tolist()\n",
    "X = np.reshape(X, (-1, 48, 48,3))\n",
    "\n",
    "y=data['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43503b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5110f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# train_datagen=ImageDataGenerator(rescale=1/255)\n",
    "# train_generator_age=train_datagen.flow(\n",
    "#     X_train ,y_train ,batch_size=32)\n",
    "\n",
    "# test_datagen=ImageDataGenerator(rescale=1/255)\n",
    "# test_generator_age=test_datagen.flow(\n",
    "#     X_test ,y_test ,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a9196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(numb_int, numb_out):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(48,48,numb_int)))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "   \n",
    "    model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))          \n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='relu'))\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463a3e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = initialize_model(X.shape[-1], y.shape[-1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a62db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de183bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='mae', patience=6, restore_best_weights=True)\n",
    "\n",
    "# earlystop=EarlyStopping(patience=6)\n",
    "# learning_rate_reduction=ReduceLROnPlateau(\n",
    "#     monitor='val_acc',\n",
    "#     patience= 3,\n",
    "#     verbose=1,\n",
    "# )\n",
    "# callbacks = [earlystop, learning_rate_reduction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66b1fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_age = model.fit(\n",
    "#     train_generator_age, \n",
    "#     epochs= 60,\n",
    "#     validation_data= test_generator_age,\n",
    "#     callbacks= callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model.fit(X_train, y_train, epochs=40, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d014940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X, y, validation_split=0.3, epochs=40, callbacks=[es], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ca500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#history.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0f22a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss']);\n",
    "plt.plot(history.history['mae']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d35cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.save_model(model, 'Model48_linearColor')\n",
    "#model = models.load_model('Model48')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986d211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b4158",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=142\n",
    "plt.imshow(X[n] );\n",
    "data.iloc[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c234dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out= model.predict(X_test)\n",
    "try_inp = np.expand_dims(X[n], axis=0)\n",
    "model.predict(try_inp)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f0f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01508e8a",
   "metadata": {},
   "source": [
    "# MODEL USING DATA BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbf9a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers, models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57b0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 5\n",
    "input_list = data['age']\n",
    "cat = age_categorize(input_list, step_size)\n",
    "#pd.DataFrame(cat, data['age'].values).sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ea20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add categorical age clasification to original dataframe\n",
    "data['class_age']=cat\n",
    "#data[['age','class_age']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4443bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data['class_age']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df7c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "### perform one-hot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse = False)\n",
    "ohe.fit(data[['class_age']])\n",
    "class_age_encoded = ohe.transform(data[['class_age']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9051a97",
   "metadata": {},
   "source": [
    " Using Pierre distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39f262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pierre distribution\n",
    "data_clean = data.copy()\n",
    "data['points_bin'] = pd.qcut(data_clean['age'], q=10)\n",
    "\n",
    "#view updated DataFrame\n",
    "print(data)\n",
    "data['points_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one-hot encoder to the Pierre distribution\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse = False)\n",
    "ohe.fit(data[['points_bin']])\n",
    "class_age_encoded = ohe.transform(data[['points_bin']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418f2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elements in range(class_age_encoded.shape[1]):\n",
    "    data[str(elements)]=class_age_encoded[:,elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a0cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data.drop(columns=['age','ethnicity','gender', 'pixels', 'points_bin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add11fe",
   "metadata": {},
   "source": [
    "Finished Pierre encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d472cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y=data.drop(columns=['age','ethnicity','gender', 'pixels', 'class_age'])\n",
    "#y=data.drop(columns=['age','ethnicity','gender', 'img_name', 'pixels', 'points_bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4a07d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['pixels'].tolist()\n",
    "X = np.reshape(X, (-1, 48, 48,3))\n",
    "\n",
    "y = class_age_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b1c397",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d2766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_catgorical(numb_int, numb_out):\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32,(3,3), padding='same',activation='relu',input_shape=(48,48,numb_int)))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    model.add(layers.Conv2D(32,(3,3), padding='same',activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    model.add(layers.Conv2D(64,(3,3), padding='same',activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128,activation='relu'))\n",
    "   \n",
    "    #model.add(layers.Flatten())\n",
    "    #model.add(layers.Dense(128,activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(numb_out, activation='softmax'))   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b5318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = initialize_model_catgorical()\n",
    "model = initialize_model_catgorical(X.shape[-1], y.shape[-1])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam' ,loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d3c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='accuracy', patience=6, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda9868",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cat = model.fit(X_train,y_train, validation_split=0.3, epochs=50, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afad490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#history_cat = model.fit(X, y, validation_split=0.3, epochs=40, callbacks=[es], batch_size=32)  # the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6883912",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.save_model(model, 'Model48_categorical_ColorData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_cat.history['val_accuracy']);\n",
    "plt.plot(history_cat.history['accuracy']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dda7242",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece4e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=244\n",
    "plt.imshow(X[n], cmap='gray');\n",
    "#np.where(y.iloc[n]==1)[0]\n",
    "print(f\"real age is {data.iloc[n]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed54e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_inp = np.expand_dims(X[n], axis=0)\n",
    "model.predict(try_inp).max()\n",
    "index = np.where(model.predict(try_inp)==(model.predict(try_inp).max()))\n",
    "index[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ded3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(try_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837fcf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_inp = np.expand_dims(X[n], axis=0)\n",
    "model.predict(try_inp).max()\n",
    "index = np.where(model.predict(try_inp)==(model.predict(try_inp).max()))\n",
    "print(f'slot number {index[1][0]}, correspond to range {index[1][0]*step_size-step_size} to {index[1][0]*step_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64598d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# best results using colab with regression on virgen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bddf859",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import csv\n",
    "\n",
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive')\n",
    "data=pd.read_csv('gdrive/My Drive/age_gender.csv')\n",
    "\n",
    "data['pixels']=data['pixels'].apply(lambda x:  np.array(x.split(), dtype=\"float32\"))\n",
    "\n",
    "from tensorflow.keras import Sequential, layers, models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['pixels'].tolist()\n",
    "X = np.reshape(X, (-1, 48, 48,1))\n",
    "\n",
    "y = data['age']\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(48,48,1)))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "   \n",
    "    model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))          \n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='relu'))\n",
    "   \n",
    "    return model\n",
    "\n",
    "model = initialize_model()\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, callbacks=[es])\n",
    "\n",
    "plt.plot(history.history['loss']);\n",
    "plt.plot(history.history['mae']);\n",
    "\n",
    "n=5\n",
    "out = np.reshape(X_test[n], (48, 48))\n",
    "plt.imshow(out, cmap='gray');\n",
    "y_test.iloc[n]\n",
    "\n",
    "try_inp = np.expand_dims(X_test[n], axis=0)\n",
    "model.predict(try_inp)[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4d127",
   "metadata": {},
   "source": [
    "# Combination of categorical plus linear for BW images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01b6e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import csv\n",
    "from tensorflow.keras import Sequential, layers, models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e7299bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_regression():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(48,48,1)))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "   \n",
    "    model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(0.2))          \n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='relu'))\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bd67a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_catgorical(numb_int, numb_out):\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32,(3,3), padding='same',activation='relu',input_shape=(48,48,numb_int)))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    model.add(layers.Conv2D(32,(3,3), padding='same',activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    model.add(layers.Conv2D(64,(3,3), padding='same',activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128,activation='relu'))\n",
    "   \n",
    "    #model.add(layers.Flatten())\n",
    "    #model.add(layers.Dense(128,activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(numb_out, activation='softmax'))   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd26d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/gonzalez/Desktop/age_gender/age_gender.csv' # old data\n",
    "data = pd.read_csv(data_path)\n",
    "data['pixels']=data['pixels'].apply(lambda x:  np.array(x.split(), dtype=\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dc70d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       age  ethnicity  gender                        img_name  \\\n",
      "0        1          2       0  20161219203650636.jpg.chip.jpg   \n",
      "1        1          2       0  20161219222752047.jpg.chip.jpg   \n",
      "2        1          2       0  20161219222832191.jpg.chip.jpg   \n",
      "3        1          2       0  20161220144911423.jpg.chip.jpg   \n",
      "4        1          2       0  20161220144914327.jpg.chip.jpg   \n",
      "...    ...        ...     ...                             ...   \n",
      "23700   99          0       1  20170120221920654.jpg.chip.jpg   \n",
      "23701   99          1       1  20170120134639935.jpg.chip.jpg   \n",
      "23702   99          2       1  20170110182418864.jpg.chip.jpg   \n",
      "23703   99          2       1  20170117195405372.jpg.chip.jpg   \n",
      "23704   99          0       1  20170110182052119.jpg.chip.jpg   \n",
      "\n",
      "                                                  pixels     points_bin  \n",
      "0      [129.0, 128.0, 128.0, 126.0, 127.0, 130.0, 133...  (0.999, 26.0]  \n",
      "1      [164.0, 74.0, 111.0, 168.0, 169.0, 171.0, 175....  (0.999, 26.0]  \n",
      "2      [67.0, 70.0, 71.0, 70.0, 69.0, 67.0, 70.0, 79....  (0.999, 26.0]  \n",
      "3      [193.0, 197.0, 198.0, 200.0, 199.0, 200.0, 202...  (0.999, 26.0]  \n",
      "4      [202.0, 205.0, 209.0, 210.0, 209.0, 209.0, 210...  (0.999, 26.0]  \n",
      "...                                                  ...            ...  \n",
      "23700  [127.0, 100.0, 94.0, 81.0, 77.0, 77.0, 74.0, 9...  (37.0, 116.0]  \n",
      "23701  [23.0, 28.0, 32.0, 35.0, 42.0, 47.0, 68.0, 85....  (37.0, 116.0]  \n",
      "23702  [59.0, 50.0, 37.0, 40.0, 34.0, 19.0, 30.0, 101...  (37.0, 116.0]  \n",
      "23703  [45.0, 108.0, 120.0, 156.0, 206.0, 197.0, 140....  (37.0, 116.0]  \n",
      "23704  [156.0, 161.0, 160.0, 165.0, 170.0, 173.0, 166...  (37.0, 116.0]  \n",
      "\n",
      "[23705 rows x 6 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.999, 26.0]    9834\n",
       "(37.0, 116.0]    7822\n",
       "(26.0, 37.0]     6049\n",
       "Name: points_bin, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pierre distribution\n",
    "data_clean = data.copy()\n",
    "data['points_bin'] = pd.qcut(data_clean['age'], q=3)\n",
    "\n",
    "#view updated DataFrame\n",
    "print(data)\n",
    "data['points_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32f765eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one-hot encoder to the Pierre distribution\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse = False)\n",
    "ohe.fit(data[['points_bin']])\n",
    "class_age_encoded = ohe.transform(data[['points_bin']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4439651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elements in range(class_age_encoded.shape[1]):\n",
    "    data[str(elements)]=class_age_encoded[:,elements]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d84f9",
   "metadata": {},
   "source": [
    "## categorical fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ddfe80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['pixels'].tolist()\n",
    "X = np.reshape(X, (-1, 48, 48,1))\n",
    "\n",
    "y=data.drop(columns=['age','ethnicity','gender', 'pixels', 'points_bin', 'img_name'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bece510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "363/363 [==============================] - 18s 48ms/step - loss: 1.8089 - accuracy: 0.5060 - val_loss: 0.9269 - val_accuracy: 0.5655\n",
      "Epoch 2/50\n",
      "363/363 [==============================] - 17s 47ms/step - loss: 0.8885 - accuracy: 0.5842 - val_loss: 0.8414 - val_accuracy: 0.6067\n",
      "Epoch 3/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.8124 - accuracy: 0.6213 - val_loss: 0.8390 - val_accuracy: 0.6175\n",
      "Epoch 4/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.7820 - accuracy: 0.6406 - val_loss: 0.8018 - val_accuracy: 0.6209\n",
      "Epoch 5/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.7523 - accuracy: 0.6522 - val_loss: 0.7914 - val_accuracy: 0.6306\n",
      "Epoch 6/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.7177 - accuracy: 0.6666 - val_loss: 0.8440 - val_accuracy: 0.6029\n",
      "Epoch 7/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.6871 - accuracy: 0.6828 - val_loss: 0.8123 - val_accuracy: 0.6382\n",
      "Epoch 8/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.6577 - accuracy: 0.7061 - val_loss: 0.8220 - val_accuracy: 0.6195\n",
      "Epoch 9/50\n",
      "363/363 [==============================] - 17s 45ms/step - loss: 0.6397 - accuracy: 0.7098 - val_loss: 0.7848 - val_accuracy: 0.6360\n",
      "Epoch 10/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.5997 - accuracy: 0.7322 - val_loss: 0.7936 - val_accuracy: 0.6438\n",
      "Epoch 11/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.5844 - accuracy: 0.7451 - val_loss: 0.8299 - val_accuracy: 0.6346\n",
      "Epoch 12/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.5387 - accuracy: 0.7639 - val_loss: 0.9554 - val_accuracy: 0.6239\n",
      "Epoch 13/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.5184 - accuracy: 0.7756 - val_loss: 0.9552 - val_accuracy: 0.6223\n",
      "Epoch 14/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.4871 - accuracy: 0.7938 - val_loss: 0.9342 - val_accuracy: 0.6362\n",
      "Epoch 15/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.4703 - accuracy: 0.8004 - val_loss: 1.0189 - val_accuracy: 0.6215\n",
      "Epoch 16/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.4234 - accuracy: 0.8198 - val_loss: 1.0423 - val_accuracy: 0.6189\n",
      "Epoch 17/50\n",
      "363/363 [==============================] - 16s 45ms/step - loss: 0.3903 - accuracy: 0.8364 - val_loss: 1.0662 - val_accuracy: 0.6310\n",
      "Epoch 18/50\n",
      "363/363 [==============================] - 17s 47ms/step - loss: 0.3531 - accuracy: 0.8547 - val_loss: 1.1559 - val_accuracy: 0.6282\n",
      "Epoch 19/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.3267 - accuracy: 0.8678 - val_loss: 1.3279 - val_accuracy: 0.6129\n",
      "Epoch 20/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.3215 - accuracy: 0.8730 - val_loss: 1.3495 - val_accuracy: 0.6053\n",
      "Epoch 21/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.2859 - accuracy: 0.8845 - val_loss: 1.4127 - val_accuracy: 0.6187\n",
      "Epoch 22/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.2808 - accuracy: 0.8898 - val_loss: 1.5309 - val_accuracy: 0.6167\n",
      "Epoch 23/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.2528 - accuracy: 0.9040 - val_loss: 1.7222 - val_accuracy: 0.6239\n",
      "Epoch 24/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.2141 - accuracy: 0.9173 - val_loss: 1.7846 - val_accuracy: 0.6165\n",
      "Epoch 25/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.2157 - accuracy: 0.9198 - val_loss: 1.7348 - val_accuracy: 0.6203\n",
      "Epoch 26/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.1748 - accuracy: 0.9353 - val_loss: 1.9651 - val_accuracy: 0.6247\n",
      "Epoch 27/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.1706 - accuracy: 0.9351 - val_loss: 2.0941 - val_accuracy: 0.6181\n",
      "Epoch 28/50\n",
      "363/363 [==============================] - 16s 45ms/step - loss: 0.1760 - accuracy: 0.9345 - val_loss: 1.9567 - val_accuracy: 0.6149\n",
      "Epoch 29/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.1798 - accuracy: 0.9353 - val_loss: 2.0479 - val_accuracy: 0.6175\n",
      "Epoch 30/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.1767 - accuracy: 0.9383 - val_loss: 2.1988 - val_accuracy: 0.6193\n",
      "Epoch 31/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.1434 - accuracy: 0.9458 - val_loss: 2.3330 - val_accuracy: 0.6189\n",
      "Epoch 32/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.1521 - accuracy: 0.9451 - val_loss: 2.1351 - val_accuracy: 0.6133\n",
      "Epoch 33/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.1532 - accuracy: 0.9470 - val_loss: 2.2808 - val_accuracy: 0.5910\n",
      "Epoch 34/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.1349 - accuracy: 0.9526 - val_loss: 2.4677 - val_accuracy: 0.6163\n",
      "Epoch 35/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.1096 - accuracy: 0.9644 - val_loss: 2.5133 - val_accuracy: 0.6073\n",
      "Epoch 36/50\n",
      "363/363 [==============================] - 17s 47ms/step - loss: 0.1296 - accuracy: 0.9550 - val_loss: 2.5809 - val_accuracy: 0.6079\n",
      "Epoch 37/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.1087 - accuracy: 0.9616 - val_loss: 2.5445 - val_accuracy: 0.6191\n",
      "Epoch 38/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.1365 - accuracy: 0.9528 - val_loss: 2.5120 - val_accuracy: 0.6097\n",
      "Epoch 39/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.1093 - accuracy: 0.9619 - val_loss: 2.5213 - val_accuracy: 0.6185\n",
      "Epoch 40/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.0786 - accuracy: 0.9725 - val_loss: 2.9024 - val_accuracy: 0.6217\n",
      "Epoch 41/50\n",
      "363/363 [==============================] - 16s 45ms/step - loss: 0.0831 - accuracy: 0.9723 - val_loss: 2.8672 - val_accuracy: 0.6195\n",
      "Epoch 42/50\n",
      "363/363 [==============================] - 16s 45ms/step - loss: 0.1401 - accuracy: 0.9561 - val_loss: 2.8762 - val_accuracy: 0.6063\n",
      "Epoch 43/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.0815 - accuracy: 0.9735 - val_loss: 3.1455 - val_accuracy: 0.6067\n",
      "Epoch 44/50\n",
      "363/363 [==============================] - 16s 45ms/step - loss: 0.1147 - accuracy: 0.9617 - val_loss: 2.6865 - val_accuracy: 0.5956\n",
      "Epoch 45/50\n",
      "363/363 [==============================] - 16s 45ms/step - loss: 0.1064 - accuracy: 0.9650 - val_loss: 3.3637 - val_accuracy: 0.5998\n",
      "Epoch 46/50\n",
      "363/363 [==============================] - 17s 48ms/step - loss: 0.1178 - accuracy: 0.9615 - val_loss: 3.1236 - val_accuracy: 0.6149\n",
      "Epoch 47/50\n",
      "363/363 [==============================] - 17s 46ms/step - loss: 0.0780 - accuracy: 0.9737 - val_loss: 3.5273 - val_accuracy: 0.6027\n",
      "Epoch 48/50\n",
      "363/363 [==============================] - 17s 46ms/step - loss: 0.0763 - accuracy: 0.9742 - val_loss: 3.0772 - val_accuracy: 0.6205\n",
      "Epoch 49/50\n",
      "363/363 [==============================] - 16s 45ms/step - loss: 0.0992 - accuracy: 0.9685 - val_loss: 3.0052 - val_accuracy: 0.6109\n",
      "Epoch 50/50\n",
      "363/363 [==============================] - 16s 44ms/step - loss: 0.0593 - accuracy: 0.9788 - val_loss: 3.5008 - val_accuracy: 0.6151\n"
     ]
    }
   ],
   "source": [
    "model_cat = initialize_model_catgorical(X.shape[-1], y.shape[-1])\n",
    "    \n",
    "es = EarlyStopping(monitor='accuracy', patience=6, restore_best_weights=True)\n",
    "\n",
    "model_cat.compile(optimizer='adam' ,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history_cat = model_cat.fit(X_train,y_train, validation_split=0.3, epochs=50, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling once the categorical branch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e9d72e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20848/1538569307.py:12: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  cacho = data.drop(data[data.age<lower].index | data[data.age>=upper].index).copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "*****************************************************************************************\n",
      "STARTING MODEL =======>>>>>> 0 with age range 0.999 to 26.0 and (7637, 48, 48, 1) samples\n",
      "*****************************************************************************************\n",
      " \n",
      "Epoch 1/40\n",
      "168/168 [==============================] - 8s 43ms/step - loss: 268.0694 - mae: 13.5359\n",
      "Epoch 2/40\n",
      "168/168 [==============================] - 7s 43ms/step - loss: 265.8198 - mae: 13.4939\n",
      "Epoch 3/40\n",
      "168/168 [==============================] - 7s 43ms/step - loss: 265.8198 - mae: 13.4939\n",
      "Epoch 4/40\n",
      "168/168 [==============================] - 7s 43ms/step - loss: 265.8198 - mae: 13.4939\n",
      "Epoch 5/40\n",
      "168/168 [==============================] - 7s 44ms/step - loss: 265.8198 - mae: 13.4939\n",
      "Epoch 6/40\n",
      "168/168 [==============================] - 7s 43ms/step - loss: 265.8198 - mae: 13.4939\n",
      "Epoch 7/40\n",
      "168/168 [==============================] - 7s 44ms/step - loss: 265.8198 - mae: 13.4939\n",
      "Epoch 8/40\n",
      "168/168 [==============================] - 7s 43ms/step - loss: 265.8198 - mae: 13.4939\n",
      "INFO:tensorflow:Assets written to: Model_linear_0.999_26.0/assets\n",
      " \n",
      "*****************************************************************************************\n",
      "STARTING MODEL =======>>>>>> 1 with age range 37.0 to 116.0 and (8111, 48, 48, 1) samples\n",
      "*****************************************************************************************\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20848/1538569307.py:12: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  cacho = data.drop(data[data.age<lower].index | data[data.age>=upper].index).copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 437.9167 - mae: 16.1410\n",
      "Epoch 2/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 345.5443 - mae: 14.7022\n",
      "Epoch 3/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 317.5146 - mae: 13.9987\n",
      "Epoch 4/40\n",
      "178/178 [==============================] - 8s 44ms/step - loss: 304.9361 - mae: 13.7118\n",
      "Epoch 5/40\n",
      "178/178 [==============================] - 8s 46ms/step - loss: 281.1828 - mae: 13.0984\n",
      "Epoch 6/40\n",
      "178/178 [==============================] - 8s 46ms/step - loss: 277.7785 - mae: 13.0717\n",
      "Epoch 7/40\n",
      "178/178 [==============================] - 8s 45ms/step - loss: 262.0753 - mae: 12.6024\n",
      "Epoch 8/40\n",
      "178/178 [==============================] - 8s 45ms/step - loss: 233.8088 - mae: 11.9434\n",
      "Epoch 9/40\n",
      "178/178 [==============================] - 9s 48ms/step - loss: 241.8948 - mae: 12.2023\n",
      "Epoch 10/40\n",
      "178/178 [==============================] - 9s 48ms/step - loss: 232.9355 - mae: 11.9309\n",
      "Epoch 11/40\n",
      "178/178 [==============================] - 8s 44ms/step - loss: 214.8554 - mae: 11.4585\n",
      "Epoch 12/40\n",
      "178/178 [==============================] - 8s 44ms/step - loss: 216.4487 - mae: 11.5587\n",
      "Epoch 13/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 205.0231 - mae: 11.1465\n",
      "Epoch 14/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 197.5701 - mae: 10.9670\n",
      "Epoch 15/40\n",
      "178/178 [==============================] - 8s 44ms/step - loss: 196.5378 - mae: 10.9298\n",
      "Epoch 16/40\n",
      "178/178 [==============================] - 9s 53ms/step - loss: 192.4034 - mae: 10.8014\n",
      "Epoch 17/40\n",
      "178/178 [==============================] - 8s 44ms/step - loss: 180.3295 - mae: 10.4048\n",
      "Epoch 18/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 177.9018 - mae: 10.4297\n",
      "Epoch 19/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 188.0964 - mae: 10.6901\n",
      "Epoch 20/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 176.6871 - mae: 10.4113\n",
      "Epoch 21/40\n",
      "178/178 [==============================] - 9s 50ms/step - loss: 174.3005 - mae: 10.3127\n",
      "Epoch 22/40\n",
      "178/178 [==============================] - 8s 45ms/step - loss: 160.8762 - mae: 9.9525\n",
      "Epoch 23/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 163.8140 - mae: 9.9312\n",
      "Epoch 24/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 162.5224 - mae: 9.9469\n",
      "Epoch 25/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 160.7443 - mae: 9.8968\n",
      "Epoch 26/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 159.6406 - mae: 9.8282\n",
      "Epoch 27/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 152.2660 - mae: 9.6673\n",
      "Epoch 28/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 146.2660 - mae: 9.4639\n",
      "Epoch 29/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 145.0329 - mae: 9.4055\n",
      "Epoch 30/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 149.4170 - mae: 9.5268\n",
      "Epoch 31/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 148.0666 - mae: 9.4473\n",
      "Epoch 32/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 145.5602 - mae: 9.4289\n",
      "Epoch 33/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 142.4076 - mae: 9.3331\n",
      "Epoch 34/40\n",
      "178/178 [==============================] - 9s 50ms/step - loss: 137.9837 - mae: 9.1694\n",
      "Epoch 35/40\n",
      "178/178 [==============================] - 9s 52ms/step - loss: 141.2941 - mae: 9.2146\n",
      "Epoch 36/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 135.2096 - mae: 9.1361\n",
      "Epoch 37/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 136.1003 - mae: 9.0996\n",
      "Epoch 38/40\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 135.5849 - mae: 9.0781\n",
      "Epoch 39/40\n",
      "178/178 [==============================] - 10s 55ms/step - loss: 140.2374 - mae: 9.2710\n",
      "Epoch 40/40\n",
      "178/178 [==============================] - 9s 50ms/step - loss: 132.4511 - mae: 8.9336\n",
      "INFO:tensorflow:Assets written to: Model_linear_37.0_116.0/assets\n",
      " \n",
      "*****************************************************************************************\n",
      "STARTING MODEL =======>>>>>> 2 with age range 26.0 to 37.0 and (7953, 48, 48, 1) samples\n",
      "*****************************************************************************************\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20848/1538569307.py:12: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  cacho = data.drop(data[data.age<lower].index | data[data.age>=upper].index).copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "174/174 [==============================] - 9s 47ms/step - loss: 104.5834 - mae: 7.6702\n",
      "Epoch 2/40\n",
      "174/174 [==============================] - 9s 50ms/step - loss: 59.3100 - mae: 6.2168\n",
      "Epoch 3/40\n",
      "174/174 [==============================] - 8s 46ms/step - loss: 53.4744 - mae: 5.8902\n",
      "Epoch 4/40\n",
      "174/174 [==============================] - 8s 45ms/step - loss: 52.4514 - mae: 5.7702\n",
      "Epoch 5/40\n",
      "174/174 [==============================] - 8s 45ms/step - loss: 48.8493 - mae: 5.5649\n",
      "Epoch 6/40\n",
      "174/174 [==============================] - 9s 50ms/step - loss: 45.5269 - mae: 5.3647\n",
      "Epoch 7/40\n",
      "174/174 [==============================] - 8s 45ms/step - loss: 43.3632 - mae: 5.2801\n",
      "Epoch 8/40\n",
      "174/174 [==============================] - 9s 49ms/step - loss: 45.2526 - mae: 5.3902\n",
      "Epoch 9/40\n",
      "174/174 [==============================] - 9s 54ms/step - loss: 42.7250 - mae: 5.2159\n",
      "Epoch 10/40\n",
      "174/174 [==============================] - 8s 47ms/step - loss: 40.0126 - mae: 5.0734\n",
      "Epoch 11/40\n",
      "174/174 [==============================] - 7s 43ms/step - loss: 38.7323 - mae: 4.9860\n",
      "Epoch 12/40\n",
      "174/174 [==============================] - 8s 44ms/step - loss: 37.4500 - mae: 4.8689\n",
      "Epoch 13/40\n",
      "174/174 [==============================] - 8s 43ms/step - loss: 36.6589 - mae: 4.8157\n",
      "Epoch 14/40\n",
      "174/174 [==============================] - 8s 46ms/step - loss: 34.6246 - mae: 4.6999\n",
      "Epoch 15/40\n",
      "174/174 [==============================] - 8s 43ms/step - loss: 31.4275 - mae: 4.4809\n",
      "Epoch 16/40\n",
      "174/174 [==============================] - 8s 48ms/step - loss: 31.4702 - mae: 4.4921\n",
      "Epoch 17/40\n",
      "174/174 [==============================] - 8s 44ms/step - loss: 31.3697 - mae: 4.4736\n",
      "Epoch 18/40\n",
      "174/174 [==============================] - 8s 45ms/step - loss: 31.1387 - mae: 4.4875\n",
      "Epoch 19/40\n",
      "174/174 [==============================] - 8s 43ms/step - loss: 29.7267 - mae: 4.3378\n",
      "Epoch 20/40\n",
      "174/174 [==============================] - 7s 43ms/step - loss: 30.8510 - mae: 4.4646\n",
      "Epoch 21/40\n",
      "174/174 [==============================] - 8s 43ms/step - loss: 29.0499 - mae: 4.3283\n",
      "Epoch 22/40\n",
      "174/174 [==============================] - 8s 43ms/step - loss: 29.6190 - mae: 4.3840\n",
      "Epoch 23/40\n",
      "174/174 [==============================] - 8s 43ms/step - loss: 29.8884 - mae: 4.3840\n",
      "Epoch 24/40\n",
      "174/174 [==============================] - 8s 43ms/step - loss: 29.3164 - mae: 4.3114\n",
      "Epoch 25/40\n",
      "174/174 [==============================] - 8s 43ms/step - loss: 29.6839 - mae: 4.3186\n",
      "Epoch 26/40\n",
      "174/174 [==============================] - 8s 46ms/step - loss: 28.9385 - mae: 4.3446\n",
      "Epoch 27/40\n",
      "174/174 [==============================] - 8s 43ms/step - loss: 28.1663 - mae: 4.2237\n",
      "Epoch 28/40\n",
      "174/174 [==============================] - 7s 43ms/step - loss: 29.0303 - mae: 4.3119\n",
      "Epoch 29/40\n",
      "174/174 [==============================] - 8s 43ms/step - loss: 28.0364 - mae: 4.2363\n",
      "Epoch 30/40\n",
      "174/174 [==============================] - 8s 43ms/step - loss: 29.7811 - mae: 4.3996\n",
      "Epoch 31/40\n",
      "174/174 [==============================] - 8s 44ms/step - loss: 29.0378 - mae: 4.3458\n",
      "Epoch 32/40\n",
      "174/174 [==============================] - 8s 43ms/step - loss: 28.6130 - mae: 4.3095\n",
      "Epoch 33/40\n",
      "174/174 [==============================] - 7s 43ms/step - loss: 28.2994 - mae: 4.2560\n",
      "INFO:tensorflow:Assets written to: Model_linear_26.0_37.0/assets\n"
     ]
    }
   ],
   "source": [
    "# looping for all linear model\n",
    "Histories = []\n",
    "\n",
    "y = data['age']\n",
    "\n",
    "for i in range(len(data['points_bin'].unique())):\n",
    "\n",
    "\n",
    "    # slice the dataframe\n",
    "    lower = data['points_bin'].unique()[i].left\n",
    "    upper = data['points_bin'].unique()[i].right\n",
    "    cacho = data.drop(data[data.age<lower].index | data[data.age>=upper].index).copy()\n",
    "    \n",
    "    # prepare the data\n",
    "    X = cacho['pixels'].tolist()\n",
    "    X = np.reshape(X, (-1, 48, 48,1))\n",
    "\n",
    "    y=cacho['age']\n",
    "\n",
    "    # split data set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "    \n",
    "    \n",
    "    print(' ')\n",
    "    print('*****************************************************************************************')\n",
    "    print(f'STARTING MODEL =======>>>>>> {i} with age range {lower} to {upper} and {X.shape} samples')\n",
    "    print('*****************************************************************************************')\n",
    "    print(' ')\n",
    "\n",
    "\n",
    "    # initialize the model\n",
    "    model = initialize_model_regression()\n",
    "        \n",
    "    # compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    # early stopping\n",
    "    es = EarlyStopping(monitor='mae', patience=6, restore_best_weights=True)\n",
    "\n",
    "\n",
    "    # fit\n",
    "    history = model.fit(X_train, y_train,  epochs=40, callbacks=[es])\n",
    "    \n",
    "    # save model\n",
    "    Histories.append(history)\n",
    "    \n",
    "    models.save_model(model, f'Model_linear_{lower}_{upper}')\n",
    "    \n",
    "    # delete variables to save RAM\n",
    "    del model, X, y, X_train, X_test, y_train, y_test, es, history, cacho\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870e4f17",
   "metadata": {},
   "source": [
    "## Evaluation test categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b247360",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['pixels'].tolist()\n",
    "X = np.reshape(X, (-1, 48, 48,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e33abb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real age is age                                                          66\n",
      "ethnicity                                                     0\n",
      "gender                                                        1\n",
      "img_name                         20170110140814578.jpg.chip.jpg\n",
      "pixels        [117.0, 102.0, 106.0, 84.0, 102.0, 94.0, 99.0,...\n",
      "points_bin                                        (37.0, 116.0]\n",
      "0                                                           0.0\n",
      "1                                                           0.0\n",
      "2                                                           1.0\n",
      "Name: 21527, dtype: object\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=21527\n",
    "plt.imshow(X[n], cmap='gray');\n",
    "#np.where(y.iloc[n]==1)[0]\n",
    "print(f\"real age is {data.iloc[n]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2086f798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99915373"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_inp = np.expand_dims(X[n], axis=0)\n",
    "model_cat.predict(try_inp).max()\n",
    "#index = np.where(model_cat.predict(try_inp)==(model_cat.predict(try_inp).max()))\n",
    "#print(f'slot number {index[1][0]}, correspond to range {index[1][0]*step_size-step_size} to {index[1][0]*step_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "22747822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.7270298e-04, 5.7354954e-04, 9.9915373e-01]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cat.predict(try_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0a8bf66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_inp = np.expand_dims(X[n], axis=0)\n",
    "model_cat.predict(try_inp).max()\n",
    "index = np.where(model_cat.predict(try_inp)==(model_cat.predict(try_inp).max()))\n",
    "index[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8370a375",
   "metadata": {},
   "source": [
    "## Evaluation test regresional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c94327e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_gender.csv:Zone.Identifier\tModel48_datafiltered\r\n",
      "images\t\t\t\tModel48_linearColor\r\n",
      "Javier_age.ipynb\t\tModel_linear_0.999_26.0\r\n",
      "Model48\t\t\t\tModel_linear_26.0_37.0\r\n",
      "Model48_categorical5\t\tModel_linear_37.0_116.0\r\n",
      "Model48_categorical_ColorData\tPaul_Gender.ipynb\r\n",
      "Model48_categorical_pierre\tPierre-Ethnicity.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "66c4ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corresponding regresional model\n",
    "predict_model = models.load_model('Model_linear_37.0_116.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "464f0f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[48.85167]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict regresional\n",
    "predict_model.predict(try_inp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
